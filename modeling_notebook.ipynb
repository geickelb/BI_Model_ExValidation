{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, accuracy_score, auc, precision_recall_fscore_support, pairwise, f1_score, log_loss, make_scorer\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import validation\n",
    "from scipy.sparse import issparse\n",
    "from scipy.spatial import distance\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import pickle\n",
    "\n",
    "\n",
    "RANDOM_STATE = 15485867\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_fxn import evaluate, hypertuning_fxn, hypertuned_cv_fxn, classifier_eval, plot_roc, optimal_youden_index, saveplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# general configs #################\n",
    "## savepath configs: ##\n",
    "# date= '30_06_2022'\n",
    "# folder='24_hr_window'\n",
    "\n",
    "## crossvalidation and modeling configs ##\n",
    "nfolds=10\n",
    "scoring='roc_auc' #neg_log_loss\n",
    "n_iter=40 #for gridsearch\n",
    "gridsearch=False #gridsearch=False means it does triaged hyperparameter combinations based on some algorithm. True= tests all \n",
    "\n",
    "## refpath configs ##\n",
    "repo_path=os.getcwd() #replace with repository path.\n",
    "new_data_path= os.path.join(repo_path, 'data' )\n",
    "\n",
    "## saving bool for plot functions ##\n",
    "save_boolean=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## data import ##############\n",
    "###all NM-T + NM-C patient info:\n",
    "final_pt_df2= pd.read_csv(os.path.join(new_data_path, 'nmh_icu_info.csv')) #anonymized patient and icustay info for NM-T and NM-C\n",
    "#######NM tertiary data:\n",
    "\n",
    "x_nmedw_train= pd.read_csv(os.path.join(new_data_path, 'x_tert_train.csv'))\n",
    "y_nmedw_train= pd.read_csv(os.path.join(new_data_path, 'y_tert_train.csv'))\n",
    "icu_nmedw_train= pd.read_csv(os.path.join(new_data_path, 'icustay_tert_train.csv'))\n",
    "\n",
    "x_nmedw_test= pd.read_csv(os.path.join(new_data_path, 'x_tert_test.csv'))\n",
    "y_nmedw_test= pd.read_csv(os.path.join(new_data_path, 'y_tert_test.csv'))\n",
    "icu_nmedw_test= pd.read_csv(os.path.join(new_data_path, 'icustay_tert_test.csv'))\n",
    "\n",
    "\n",
    "#######community data:\n",
    "x_nmedw_com= pd.read_csv(os.path.join(new_data_path, 'x_com.csv'))\n",
    "y_nmedw_com= pd.read_csv(os.path.join(new_data_path, 'y_com.csv'))\n",
    "icu_nmedw_com= pd.read_csv(os.path.join(new_data_path, 'icustay_com.csv'))\n",
    "\n",
    "####MIMIC data:\n",
    "mimic_final_pt= pd.read_csv(os.path.join(new_data_path, 'mimic_icu_info.csv')) #anonymized patient and icustay info for MIMIC\n",
    "\n",
    "x_mimic_test= pd.read_csv(os.path.join(new_data_path, 'x_mimic_test.csv'))\n",
    "y_mimic_test= pd.read_csv(os.path.join(new_data_path, 'y_mimic_test.csv'))\n",
    "icu_mimic_test= pd.read_csv(os.path.join(new_data_path, 'icu_mimic_test.csv'))\n",
    "\n",
    "x_mimic_train= pd.read_csv(os.path.join(new_data_path, 'x_mimic_train.csv'))\n",
    "y_mimic_train= pd.read_csv(os.path.join(new_data_path, 'y_mimic_train.csv'))\n",
    "icu_mimic_train= pd.read_csv(os.path.join(new_data_path, 'icu_mimic_train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick dataset distribution breakdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nmedw_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nmedw_train['0'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nmedw_test['0'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mimic_train['0'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mimic_test['0'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pt_df2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_pt_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_final_pt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mimic_final_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 experiments:\n",
    "* Exp1: MIMIC trained model -> test on: MIMIC_test ; Recalibrate on Tertiary_train -> test on:NMEDW_Test\n",
    "* Exp2: Tertiary trained model -> test on: Tertiary_test ; Recalibrate on MIMIC_train -> test on:MIMIC_test\n",
    "* Exp3: Train on Pooled(NMEDW_train + MIMIC_train), test on MIMIC_test;  NMEDW_Test.\n",
    "* Exp4: Ensemble (MIMIC,NM_tert), test on MIMIC_test;  NMEDW_Test\n",
    "* Exp5: test all above models on NM Community \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp1: MIMIC trained model -> test on: MIMIC_test ; Recalibrate on Tertiary_train -> test on:NMEDW_Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(x_mimic_train.copy())\n",
    "y=y_mimic_train.copy() #copy of y_train\n",
    "y=y.astype('int')\n",
    "y=np.array(y).ravel()\n",
    "\n",
    "z_subject_id= pd.merge(pd.DataFrame(icu_mimic_train), mimic_final_pt[['icustay_id','subject_id']], how='left')['subject_id'] #7205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###rf: HYPERTUNING\n",
    "# Number of trees in random forest\n",
    "n_estimators = [25, 50, 150, 250]\n",
    "# Number of features to consider at every split\n",
    "max_features = [3,10,20,'auto']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5, 7, 10, 15] \n",
    "#max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [2, 5, 10]\n",
    "# Method of selecting samples for training each tree. supposedly better with false when classes aren't perfectly ballanced\n",
    "bootstrap = [True] \n",
    "\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "model= RandomForestClassifier(criterion='entropy', random_state=12345)\n",
    "\n",
    "rf_hyper_exp_mim=hypertuning_fxn(x, y, nfolds=nfolds, model=model , param_grid=param_grid,z_subject_id=z_subject_id, scoring=scoring,n_iter = n_iter, gridsearch=False) #2.2 min with gridsearch false. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###model fitting and calibrating to training data\n",
    "rf_mimic = CalibratedClassifierCV(rf_hyper_exp_mim.best_estimator_, method='sigmoid', cv=10)\n",
    "rf_mimic.fit(x, y)\n",
    "\n",
    "# hypertuned_cv_fxn(x, y, model_in, nfolds, lr_override=False)\n",
    "rf_hyper_exp_mim2= hypertuned_cv_fxn(np.array(x_mimic_train.copy()), y, rf_mimic, nfolds=nfolds, z_subject_id=z_subject_id, lr_override=True)\n",
    "rf_hyper_exp_mim2['tp_threshold'] #0.10776977266402202"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mimic model calibrated on mimic_train:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_model={'model':rf_mimic, \n",
    "            'threshold':rf_hyper_exp_mim2['tp_threshold'],\n",
    "            'calibration':'mimic_train'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mimic model calibrated on Tertiary_train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using the mimic target train data to find classification threshold\n",
    "x=np.array(x_nmedw_train.copy())\n",
    "y=y_nmedw_train.copy() #copy of y_train\n",
    "y=y.astype('int')\n",
    "y=np.array(y).ravel()\n",
    "z_subject_id= icu_nmedw_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### recalibrating mimic model to nmedw train:\n",
    "\n",
    "rf_mimic_recal = CalibratedClassifierCV(mimic_model['model'], method='sigmoid', cv=\"prefit\")\n",
    "rf_mimic_recal.fit(x, y)\n",
    "\n",
    "rf_mimic_recal_cv= hypertuned_cv_fxn(x, y, rf_mimic_recal, nfolds=10, lr_override=False, z_subject_id=z_subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_model_recal={'model':rf_mimic_recal, \n",
    "            'threshold':rf_mimic_recal_cv['tp_threshold'],\n",
    "            'calibration':'tert_train'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate MIMIC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('### MIMIC MODEL ON MIMIC TEST DATA')\n",
    "rf_mimic_test= classifier_eval(rf_mimic, x=np.array(x_mimic_test), y=y_mimic_test,\n",
    "                         training=False,train_threshold= rf_hyper_exp_mim2['tp_threshold'],\n",
    "                         model_name='rf', folder_name=folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MIMIC RF MODEL: NMEDW TEST')\n",
    "\n",
    "x_test=np.array(x_nmedw_test.copy())\n",
    "y_test=y_nmedw_test.copy() #copy of y_train\n",
    "y_test=y_test.astype('int')\n",
    "y_test=np.array(y_test).ravel()\n",
    "z_subject_id= icu_nmedw_test\n",
    "\n",
    "rf_mimic_eval_exp1= classifier_eval(mimic_model_recal['model'], x=np.array(x_test), y=y_test,\n",
    "                              training=False,\n",
    "                              train_threshold=mimic_model_recal['threshold'],\n",
    "                              model_name='rf_mimic_recal', folder_name='24_hr_window')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_model.update({'results_mimicD-test':rf_mimic_test})\n",
    "mimic_model_recal.update({'results_tertD-test':rf_mimic_eval_exp1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_internal= mimic_model['results_mimicD-test'].copy()\n",
    "results_internal['train_set']='MIMIC'\n",
    "results_internal['test_set']='MIMIC'\n",
    "results_internal['calibration']=mimic_model['calibration']\n",
    "\n",
    "results_external= mimic_model_recal['results_tertD-test'].copy()\n",
    "results_external['train_set']='MIMIC'\n",
    "results_external['test_set']='tert'\n",
    "results_external['calibration']=mimic_model_recal['calibration']\n",
    "\n",
    "test_summary_exp1_df= pd.DataFrame([results_internal,results_external])\n",
    "\n",
    "test_summary_exp1_df=test_summary_exp1_df.round(decimals=3).sort_values('auc', ascending=False)\n",
    "test_summary_exp1_df=test_summary_exp1_df[['train_set','test_set','calibration','auc','f1','npv','precision','recall','threshold']]\n",
    "display(test_summary_exp1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nmedw_train['0'].value_counts()/len(y_nmedw_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp2: Tertiary trained model -> test on: Tertiary_test ; Recalibrate on MIMIC_train -> test on:MIMIC_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(x_nmedw_train.copy())\n",
    "y=y_nmedw_train.copy() #copy of y_train\n",
    "y=y.astype('int')\n",
    "y=np.array(y).ravel()\n",
    "z_subject_id= pd.merge(pd.DataFrame(icu_nmedw_train), final_pt_df2[['icustay_id','subject_id']], how='left')['icustay_id'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model hypertuning using same previously instantiated hyperparameter grid\n",
    "model= RandomForestClassifier(criterion='entropy', random_state=12345)\n",
    "\n",
    "rf_hyper_exp2=hypertuning_fxn(x, y, nfolds=nfolds, model=model , param_grid=param_grid,z_subject_id=z_subject_id, scoring=scoring,n_iter = n_iter, gridsearch=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model hypertuning using same previously instantiated hyperparameter grid\n",
    "rf_tert = CalibratedClassifierCV(rf_hyper_exp2.best_estimator_, method='sigmoid', cv=10)\n",
    "rf_tert.fit(x, y)\n",
    "\n",
    "rf_tert_cv= hypertuned_cv_fxn(x, y, rf_tert, nfolds=nfolds, lr_override=True, z_subject_id=z_subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model dictionary w/ results, threshold and model object\n",
    "\n",
    "tert_model={'model':rf_tert, \n",
    "            'threshold':rf_tert_cv['tp_threshold'],\n",
    "            'calibration':'tert_train'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### recalibrating model to the respective training set for desired test set evaluation\n",
    "x=np.array(x_mimic_train.copy())\n",
    "y=y_mimic_train.copy() #copy of y_train\n",
    "y=y.astype('int')\n",
    "y=np.array(y).ravel()\n",
    "z_subject_id= pd.merge(pd.DataFrame(icu_mimic_train), mimic_final_pt[['icustay_id','subject_id']], how='left')['subject_id']\n",
    "\n",
    "\n",
    "rf_tert_recal = CalibratedClassifierCV(tert_model['model'], method='sigmoid', cv=\"prefit\")\n",
    "rf_tert_recal.fit(x, y)\n",
    "\n",
    "rf_tert_recal_cv= hypertuned_cv_fxn(x, y, rf_tert_recal, nfolds=10, lr_override=False, z_subject_id=z_subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tert_model_recal={'model':rf_tert_recal, \n",
    "            'threshold':rf_tert_recal_cv['tp_threshold'],\n",
    "            'calibration':'mimic_train'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate Tertiary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tertiary MODEL ON NMEDW TEST DATA')\n",
    "tert_eval_tert= classifier_eval(tert_model['model'], x=np.array(x_nmedw_test), y=y_nmedw_test,\n",
    "                         training=False,train_threshold= tert_model['threshold'],\n",
    "                         model_name='rf', folder_name=folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tertiary MODEL ON MIMIC TEST DATA')\n",
    "tert_eval_mimic_test= classifier_eval(tert_model_recal['model'], x=np.array(x_mimic_test), y=y_mimic_test,\n",
    "                         training=False,train_threshold= tert_model_recal['threshold'],\n",
    "                         model_name='rf_recal', folder_name=folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tert_model.update({'results_tertD-test':tert_eval_tert})\n",
    "tert_model_recal.update({'results_mimicD-test':tert_eval_mimic_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_internal= tert_model['results_tertD-test'].copy()\n",
    "results_internal['train_set']='TERT'\n",
    "results_internal['test_set']='TERT'\n",
    "results_internal['calibration']=tert_model['calibration']\n",
    "\n",
    "results_external= tert_model_recal['results_mimicD-test'].copy()\n",
    "results_external['train_set']='TERT'\n",
    "results_external['test_set']='MIMIC'\n",
    "results_external['calibration']=tert_model_recal['calibration']\n",
    "\n",
    "\n",
    "test_summary_exp2_df= pd.DataFrame([results_internal,results_external])\n",
    "\n",
    "test_summary_exp2_df=test_summary_exp2_df.round(decimals=3).sort_values('auc', ascending=False)\n",
    "test_summary_exp2_df=test_summary_exp2_df[['train_set','test_set','calibration','auc','f1','npv','precision','recall','threshold']]\n",
    "display(test_summary_exp2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment 3: Exp3: Train on POOLED  (NMEDW_train + MIMIC_train), test on MIMIC_test ; NMEDW_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mim=pd.merge(icu_mimic_test, mimic_final_pt[['icustay_id','final_bin']])\n",
    "edw=pd.merge(icu_nmedw_test, final_pt_df2[['icustay_id','final_bin']])\n",
    "mim['source']='mimic'\n",
    "edw['source']='edw'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mimic_train['0'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nmedw_train['0'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merger(y_mimic_test, y_nmedw_test, x_nmedw_test,x_mimic_test, icu_mimic_test,icu_nmedw_test ):\n",
    "\n",
    "    \"\"\"\n",
    "    function to sample an equal # of samples from NM-T and MIMIC, but to preserve each dataset's BI prevalence in the process. \n",
    "    \"\"\"\n",
    "    ##counts\n",
    "    l_mim= len(y_mimic_test)\n",
    "    l_nm= len(y_nmedw_test)\n",
    "    display(l_mim, l_nm)\n",
    "\n",
    "    min_n=min(l_mim, l_nm)\n",
    "\n",
    "    if l_mim!=min_n:\n",
    "        neg_n=c1=sum(y_mimic_test['0']==0)\n",
    "        pos_n=c1=sum(y_mimic_test['0']==1)\n",
    "        neg_ratio= neg_n/len(y_mimic_test)\n",
    "\n",
    "        n_neg= round(neg_ratio*min_n)\n",
    "        n_pos= min_n-n_neg\n",
    "\n",
    "        ytest_1= y_mimic_test[y_mimic_test['0']==0].sample(n=n_neg, replace=False, random_state=12345)\n",
    "        ytest_2= y_mimic_test[y_mimic_test['0']==1].sample(n=n_pos, replace=False, random_state=12345)\n",
    "        y_pooled_test2= pd.concat([y_nmedw_test, ytest_1, ytest_2]) #[edw, mimic]\n",
    "\n",
    "\n",
    "        mim2=pd.merge(icu_mimic_test.loc[pd.concat([ytest_1, ytest_2]).index], mimic_final_pt[['icustay_id','final_bin']])\n",
    "        edw2=pd.merge(icu_nmedw_test, final_pt_df2[['icustay_id','final_bin']])\n",
    "        \n",
    "        mim2['icustay_id']=mim2['icustay_id']*100 ## doing this to make sure mimic and nmedw icustays don't overlap\n",
    "         \n",
    "        mim2['source']='mimic'\n",
    "        edw2['source']='edw'\n",
    "        icu_pooled_test2= pd.concat([edw2,mim2]) #[edw, mimic]\n",
    "\n",
    "        x_pooled_test2= pd.concat([x_nmedw_test,x_mimic_test.loc[ytest_1.index], x_mimic_test.loc[ytest_2.index]]) #[edw, mimic]\n",
    "\n",
    "        return(y_pooled_test2,x_pooled_test2,icu_pooled_test2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pooled_test2,x_pooled_test2,icu_pooled_test2 = merger(y_mimic_test, y_nmedw_test, x_nmedw_test,x_mimic_test, icu_mimic_test,icu_nmedw_test )\n",
    "\n",
    "## QC to ensure proper dimensions of pooled dataset\n",
    "print(\n",
    "len(y_pooled_test2),\n",
    "len(x_pooled_test2),\n",
    "len(icu_pooled_test2))\n",
    "icu_pooled_test2['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pooled_train2,x_pooled_train2,icu_pooled_train2 = merger(y_mimic_train, y_nmedw_train, x_nmedw_train,x_mimic_train, icu_mimic_train,icu_nmedw_train )\n",
    "print(\n",
    "len(y_pooled_train2),\n",
    "len(x_pooled_train2),\n",
    "len(icu_pooled_train2))\n",
    "icu_pooled_train2['icustay_id']=icu_pooled_train2['icustay_id']#.astype('float')\n",
    "icu_pooled_train2['source'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x=np.array(x_pooled_train2.copy())\n",
    "y=y_pooled_train2.copy() #copy of y_train\n",
    "y=y.astype('int')\n",
    "y=np.array(y).ravel()\n",
    "z_subject_id= icu_pooled_train2['icustay_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(icu_pooled_train2['icustay_id'].nunique())\n",
    "print(icu_pooled_test2['icustay_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_pooled_train2['icustay_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model hypertuning using same previously instantiated hyperparameter grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "model= RandomForestClassifier(criterion='entropy', random_state=12345)\n",
    "rf_hyper_pooled=hypertuning_fxn(x, y, nfolds=nfolds, model=model , param_grid=param_grid, z_subject_id=z_subject_id, scoring=scoring,n_iter = n_iter, gridsearch=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding this manually for the pooled hypertuning because the different anonomyzed icustay_id's for the demo are messing w/ the seed and yielding slightly different results\n",
    "rf_hyper_pooled.best_estimator_.max_features=20\n",
    "rf_hyper_pooled.best_estimator_.max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pooled = CalibratedClassifierCV(rf_hyper_pooled.best_estimator_, method='sigmoid', cv=10)\n",
    "\n",
    "rf_pooled.fit(x, y)\n",
    "\n",
    "rf_pooled_cv= hypertuned_cv_fxn(x, y, rf_pooled, nfolds=10, lr_override=True, z_subject_id=z_subject_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_model={'model':rf_pooled, \n",
    "            'threshold':rf_pooled_cv['tp_threshold'],\n",
    "            'calibration':'pooled_train'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recalibrating pooled models to each dataset prior to testing on respective test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### recalibrating model to the respective training set for desired test set evaluation\n",
    "x=np.array(x_mimic_train.copy())\n",
    "y=y_mimic_train.copy() #copy of y_train\n",
    "y=y.astype('int')\n",
    "y=np.array(y).ravel()\n",
    "z_subject_id= pd.merge(pd.DataFrame(icu_mimic_train), mimic_final_pt[['icustay_id','subject_id']], how='left')['subject_id']\n",
    "\n",
    "rf_pooled_recal_mim = CalibratedClassifierCV(pooled_model['model'], method='sigmoid', cv=\"prefit\")\n",
    "rf_pooled_recal_mim.fit(x, y)\n",
    "\n",
    "rf_pooled_recal_mim_cv= hypertuned_cv_fxn(x, y, rf_pooled_recal_mim, nfolds=10, lr_override=True, z_subject_id=z_subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model dictionary w/ results, threshold and model object\n",
    "pooled_model_recal_mim={'model':rf_pooled_recal_mim, \n",
    "            'threshold':rf_pooled_recal_mim_cv['tp_threshold'],\n",
    "            'calibration':'mim_train'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### recalibrating model to the respective training set for desired test set evaluation\n",
    "x=np.array(x_nmedw_train.copy())\n",
    "y=y_nmedw_train.copy() #copy of y_train\n",
    "y=y.astype('int')\n",
    "y=np.array(y).ravel()\n",
    "z_subject_id= pd.merge(pd.DataFrame(icu_nmedw_train), final_pt_df2[['icustay_id','subject_id']], how='left')['icustay_id'] #7205\n",
    "\n",
    "\n",
    "rf_pooled_recal_tert = CalibratedClassifierCV(pooled_model['model'], method='sigmoid', cv=\"prefit\")\n",
    "rf_pooled_recal_tert.fit(x, y)\n",
    "\n",
    "rf_pooled_recal_tert_cv= hypertuned_cv_fxn(x, y, rf_pooled_recal_tert, nfolds=10, lr_override=True, z_subject_id=z_subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model dictionary w/ results, threshold and model object\n",
    "pooled_model_recal_tert={'model':rf_pooled_recal_tert, \n",
    "            'threshold':rf_pooled_recal_tert_cv['tp_threshold'],\n",
    "            'calibration':'tert_train'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model evaluation: pooled model on mimic testset\n",
    "pooled_mim_eval= classifier_eval(pooled_model_recal_mim['model'], x=np.array(x_mimic_test), y=y_mimic_test,\n",
    "                         training=False,train_threshold= pooled_model_recal_mim['threshold'],\n",
    "                         model_name='rf', folder_name=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model evaluation: pooled model on NM-T testset\n",
    "pooled_tert_eval= classifier_eval(pooled_model_recal_tert['model'], x=np.array(x_nmedw_test), y=y_nmedw_test,\n",
    "                         training=False,train_threshold= pooled_model_recal_tert['threshold'],\n",
    "                         model_name='rf', folder_name=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_model_recal_mim.update({'results_mimicD-test':pooled_mim_eval})\n",
    "pooled_model_recal_tert.update({'results_tertD-test':pooled_tert_eval})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_external_mim= pooled_model_recal_mim['results_mimicD-test'].copy()\n",
    "results_external_mim['train_set']='POOLED'\n",
    "results_external_mim['test_set']='MIMIC'\n",
    "results_external_mim['calibration']=pooled_model_recal_mim['calibration']\n",
    "\n",
    "results_external_tert= pooled_model_recal_tert['results_tertD-test'].copy()\n",
    "results_external_tert['train_set']='POOLED'\n",
    "results_external_tert['test_set']='TERT'\n",
    "results_external_tert['calibration']=pooled_model_recal_tert['calibration']\n",
    "\n",
    "\n",
    "test_summary_exp3_df= pd.DataFrame([results_external_mim, results_external_tert])#, logreg_mimic_test,logreg_mimic_eval_exp1])\n",
    "test_summary_exp3_df=test_summary_exp3_df.round(decimals=3).sort_values('auc', ascending=False)\n",
    "test_summary_exp3_df=test_summary_exp3_df[['train_set','test_set','calibration','auc','f1','npv','precision','recall','threshold']]\n",
    "display(test_summary_exp3_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp4: Ensembling Tertiary and MIMIC models\n",
    "* recalibrate on respective training data before ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ensembling mimic tailored NM-Tm and MIMICm:\n",
    "#############fitting\n",
    "x=np.array(x_mimic_train.copy())\n",
    "y=y_mimic_train.copy() #copy of y_train\n",
    "y=y.astype('int')\n",
    "y=np.array(y).ravel()\n",
    "#time_interval=4\n",
    "z_subject_id= pd.merge(pd.DataFrame(icu_mimic_train), mimic_final_pt[['icustay_id','subject_id']], how='left')['icustay_id'] \n",
    "\n",
    "clf_list_calibrated_mim = [tert_model_recal['model'], mimic_model['model']]\n",
    "\n",
    "ensemble_mim_model = VotingClassifier(estimators = [('tert_recal' ,tert_model_recal['model']), ('mimic_cal', mimic_model['model'])], voting='soft')\n",
    "\n",
    "ensemble_mim_model.estimators_ = clf_list_calibrated_mim\n",
    "ensemble_mim_model.le_ = LabelEncoder().fit(y)\n",
    "ensemble_mim_model.classes_ = ensemble_mim_model.le_.classes_\n",
    "\n",
    "# Now it will work without calling fit\n",
    "ensemble_cal_cv_mim= hypertuned_cv_fxn(x, y, ensemble_mim_model, nfolds=10, lr_override=True, z_subject_id=z_subject_id)\n",
    "\n",
    "\n",
    "ensemble_mimic={'model':ensemble_mim_model, \n",
    "            'threshold':ensemble_cal_cv_mim['tp_threshold'],\n",
    "            'calibration':'mimic_train'}\n",
    "\n",
    "### rf: cat model on cat testset (balanced)\n",
    "ensemble_eval_mim= classifier_eval(ensemble_mimic['model'], x=np.array(x_mimic_test), y=y_mimic_test,\n",
    "                         training=False,train_threshold= ensemble_mimic['threshold'],#cv_cat_df.loc['RandomForestClassifier','tp_threshold'],\n",
    "                         model_name='ensemble_cal_mimic', folder_name=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_cal_cv_mim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mimic.update({'results_mimicD-test':ensemble_eval_mim})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ensembling NM-T tailored NM-Tm and MIMICm:\n",
    "#############fitting\n",
    "x=np.array(x_nmedw_train.copy())\n",
    "y=y_nmedw_train.copy() #copy of y_train\n",
    "y=y.astype('int')\n",
    "y=np.array(y).ravel()\n",
    "#time_interval=4\n",
    "z_subject_id= pd.merge(pd.DataFrame(icu_nmedw_train), final_pt_df2[['icustay_id','subject_id']], how='left')['icustay_id'] #7205\n",
    "\n",
    "\n",
    "clf_list_calibrated_tert= [tert_model['model'], mimic_model_recal['model']]\n",
    "\n",
    "ensemble_tert_model = VotingClassifier(estimators = [('tert_cal' ,tert_model['model']), ('mimic_recal', mimic_model_recal['model'])], voting='soft')\n",
    "\n",
    "ensemble_tert_model.estimators_ = clf_list_calibrated_tert\n",
    "ensemble_tert_model.le_ = LabelEncoder().fit(y)\n",
    "ensemble_tert_model.classes_ = ensemble_tert_model.le_.classes_\n",
    "\n",
    "# Now it will work without calling fit\n",
    "ensemble_cal_cv_tert= hypertuned_cv_fxn(x, y, ensemble_tert_model, nfolds=10, lr_override=True, z_subject_id=z_subject_id)\n",
    "\n",
    "\n",
    "ensemble_tert={'model':ensemble_tert_model, \n",
    "            'threshold':ensemble_cal_cv_tert['tp_threshold'],\n",
    "            'calibration':'tert_train'}\n",
    "\n",
    "### rf: cat model on cat testset (balanced)\n",
    "ensemble_eval_tert= classifier_eval(ensemble_tert['model'], x=np.array(x_nmedw_test), y=y_nmedw_test,\n",
    "                         training=False,train_threshold= ensemble_tert['threshold'],\n",
    "                         model_name='ensemble_cal_tert', folder_name=folder)\n",
    "\n",
    "ensemble_tert.update({'results_tertD-test':ensemble_eval_tert})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model dictionary w/ results, threshold and model object\n",
    "\n",
    "results_internal= ensemble_tert['results_tertD-test'].copy()\n",
    "results_internal['train_set']='ENSEMBLE'\n",
    "results_internal['test_set']='TERT'\n",
    "results_internal['calibration']=ensemble_tert['calibration']\n",
    "\n",
    "\n",
    "results_external= ensemble_mimic['results_mimicD-test'].copy()\n",
    "results_external['train_set']='ENSEMBLE'\n",
    "results_external['test_set']='MIMIC'\n",
    "results_external['calibration']=ensemble_mimic['calibration']\n",
    "\n",
    "test_summary_exp4_df= pd.DataFrame([results_internal,results_external])\n",
    "\n",
    "test_summary_exp4_df=test_summary_exp4_df.round(decimals=3).sort_values('auc', ascending=False)\n",
    "test_summary_exp4_df=test_summary_exp4_df[['train_set','test_set','calibration','auc','f1','npv','precision','recall','threshold']]\n",
    "display(test_summary_exp4_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# result compiling for exp 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_list=[mimic_model,\n",
    "                 mimic_model_recal,\n",
    "\n",
    "                tert_model,\n",
    "                tert_model_recal,\n",
    "\n",
    "                pooled_model,\n",
    "                pooled_model_recal_mim,\n",
    "                pooled_model_recal_tert,\n",
    "\n",
    "                ensemble_mimic,\n",
    "                ensemble_tert\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_summary_exp1_df.reset_index())\n",
    "display(test_summary_exp2_df.reset_index())\n",
    "display(test_summary_exp3_df.reset_index())\n",
    "display(test_summary_exp4_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save1(model, model_name):\n",
    "    import pickle\n",
    "    modelpath=str(repository_path)+'/models/{}_{}'.format(date,'august_final')\n",
    "\n",
    "    if not os.path.exists(modelpath):\n",
    "        print(modelpath)\n",
    "        os.makedirs(modelpath)\n",
    "\n",
    "    filename = str(modelpath)+'/calibrated_{}.sav'.format(model_name)\n",
    "    print(filename)\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "      \n",
    "###code to save models:\n",
    "# model_save1(mimic_model['model'],'rf_mimic_mim_cal')\n",
    "# model_save1(mimic_model_recal['model'],'rf_mimic_tert_recal')\n",
    "\n",
    "# model_save1(tert_model['model'],'rf_tert_tert_cal')\n",
    "# model_save1(tert_model_recal['model'],'rf_tert_mim_recal')\n",
    "\n",
    "# model_save1(pooled_model['model'],'rf_pooled_mer_cal')\n",
    "# model_save1(pooled_model_recal_mim['model'],'rf_pooled_mim_recal')\n",
    "# model_save1(pooled_model_recal_tert['model'],'rf_pooled_tert_recal')\n",
    "\n",
    "\n",
    "# model_save1(ensemble_mimic['model'],'rf_ensemble_mim_cal')\n",
    "# model_save1(ensemble_tert['model'],'rf_ensemble_tert_cal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# applying models to community hospitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(x_nmedw_com.copy())\n",
    "y=y_nmedw_com.copy() #copy of y_train\n",
    "y=y.astype('int')\n",
    "y=np.array(y).ravel()\n",
    "z_subject_id= icu_nmedw_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_list=[mimic_model,\n",
    "                 mimic_model_recal,\n",
    "\n",
    "                tert_model,\n",
    "                tert_model_recal,\n",
    "\n",
    "                pooled_model,\n",
    "                pooled_model_recal_mim,\n",
    "                pooled_model_recal_tert,\n",
    "\n",
    "                ensemble_mimic,\n",
    "                ensemble_tert\n",
    "                ]\n",
    "    \n",
    "com_model_dict_list= [mimic_model_recal,\n",
    "                      tert_model,\n",
    "                      pooled_model_recal_tert,\n",
    "                      ensemble_tert\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model evaluation of each model (calibrated to NM-Ttrain) on NM-C\n",
    "mimic_eval_com= classifier_eval(mimic_model_recal['model'], x=x, y=y,\n",
    "                         training=False,train_threshold= mimic_model_recal['threshold'],#cv_cat_df.loc['RandomForestClassifier','tp_threshold'],\n",
    "                         model_name='rf', folder_name=folder)\n",
    "\n",
    "tert_eval_com= classifier_eval(tert_model['model'], x=x, y=y,\n",
    "                         training=False,train_threshold= tert_model['threshold'],#cv_cat_df.loc['RandomForestClassifier','tp_threshold'],\n",
    "                         model_name='rf', folder_name=folder)\n",
    "\n",
    "pooled_eval_com= classifier_eval(pooled_model_recal_tert['model'], x=x, y=y,\n",
    "                         training=False,train_threshold= pooled_model_recal_tert['threshold'],#cv_cat_df.loc['RandomForestClassifier','tp_threshold'],\n",
    "                         model_name='rf', folder_name=folder)\n",
    "\n",
    "ensemble_eval_com= classifier_eval(ensemble_tert['model'], x=x, y=y,\n",
    "                         training=False,train_threshold= ensemble_tert['threshold'],#cv_cat_df.loc['RandomForestClassifier','tp_threshold'],\n",
    "                         model_name='rf', folder_name=folder)\n",
    "\n",
    "\n",
    "\n",
    "mimic_model_recal.update({'results_com':mimic_eval_com})\n",
    "tert_model.update({'results_com':tert_eval_com})\n",
    "pooled_model_recal_tert.update({'results_com':pooled_eval_com})\n",
    "ensemble_tert.update({'results_com':ensemble_eval_com})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model dictionary for each nm-c result\n",
    "\n",
    "com_model_dict_list= [mimic_model_recal,\n",
    "                      tert_model,\n",
    "                      pooled_model_recal_tert,\n",
    "                      ensemble_tert\n",
    "    ]\n",
    "i=0\n",
    "for element in com_model_dict_list:\n",
    "    print(i)\n",
    "    y_proba= element['model'].predict_proba(x_nmedw_com)[:,1]\n",
    "    element.update({'com_proba':y_proba})\n",
    "    element.update({'com_pred':[1 if y >= element['threshold'] else 0 for y in y_proba]})\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_eval_com= classifier_eval(mimic_model_recal['model'], x=x, y=y,\n",
    "                         training=False,train_threshold= mimic_model_recal['threshold'],\n",
    "                         model_name='rf', folder_name=folder)\n",
    "\n",
    "tert_eval_com= classifier_eval(tert_model['model'], x=x, y=y,\n",
    "                         training=False,train_threshold= tert_model['threshold'],\n",
    "                         model_name='rf', folder_name=folder)\n",
    "\n",
    "pooled_eval_com= classifier_eval(pooled_model_recal_tert['model'], x=x, y=y,\n",
    "                         training=False,train_threshold= pooled_model_recal_tert['threshold'],\n",
    "                         model_name='rf', folder_name=folder)\n",
    "\n",
    "ensemble_eval_com= classifier_eval(ensemble_tert['model'], x=x, y=y,\n",
    "                         training=False,train_threshold= ensemble_tert['threshold'],\n",
    "                         model_name='rf', folder_name=folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### nm-C result table: \n",
    "tert_eval_com['train_set']='NMEDW_NMH'\n",
    "tert_eval_com['test_set']='NMEDW_COMMUNITY'\n",
    "\n",
    "mimic_eval_com['train_set']='MIMIC'\n",
    "mimic_eval_com['test_set']='NMEDW_COMMUNITY'\n",
    "\n",
    "pooled_eval_com['train_set']='POOLED'\n",
    "pooled_eval_com['test_set']='NMEDW_COMMUNITY'\n",
    "\n",
    "ensemble_eval_com['train_set']='ENSEMBLE'\n",
    "ensemble_eval_com['test_set']='NMEDW_COMMUNITY'\n",
    "\n",
    "test_com_summary_df= pd.DataFrame([mimic_eval_com,tert_eval_com,pooled_eval_com,ensemble_eval_com])\n",
    "\n",
    "test_com_summary_df=test_com_summary_df.round(decimals=3)#.sort_values('auc', ascending=False)\n",
    "test_com_summary_df['model']='RandomForestClassifier'\n",
    "\n",
    "test_com_summary_df=test_com_summary_df[['train_set','test_set','auc','f1','npv','precision','recall','threshold']]\n",
    "\n",
    "display(test_com_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calibration Evaluation (calcs and plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def cal_plot(X_test, y_test,clf_list, save_bool=False, plotname='calplot'):\n",
    "    \"\"\"\n",
    "    calibration plot, taken from SKlearn documentation and modified slightly\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.calibration import CalibrationDisplay\n",
    "\n",
    "    sns.set(font_scale=2.5)\n",
    "    plt.style.use('seaborn-white')\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    gs = GridSpec(4, 2)\n",
    "    \n",
    "    from matplotlib.colors import ListedColormap\n",
    "    colors = ListedColormap(sns.color_palette())\n",
    "\n",
    "    ax_calibration_curve = fig.add_subplot(gs[:2, :2])\n",
    "    calibration_displays = {}\n",
    "    for i, (clf, name) in enumerate(clf_list):\n",
    "        display = CalibrationDisplay.from_estimator(\n",
    "            clf,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            n_bins=10,\n",
    "            name=name+'$_{\\mathrm{M}}$',\n",
    "            ax=ax_calibration_curve,\n",
    "            color=colors(i),\n",
    "        )\n",
    "        calibration_displays[name] = display\n",
    "       \n",
    "    ax_calibration_curve.grid()\n",
    "    ax_calibration_curve.legend(bbox_to_anchor=(1.05, 0.45), loc='upper left', borderaxespad=0)\n",
    "\n",
    "    # Add histogram\n",
    "    grid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\n",
    "    for i, (_, name) in enumerate(clf_list):\n",
    "        row, col = grid_positions[i]\n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "\n",
    "        ax.hist(\n",
    "            calibration_displays[name].y_prob,\n",
    "            range=(0, 1),\n",
    "            bins=10,\n",
    "            label=name,\n",
    "            color=colors(i),\n",
    "        )\n",
    "        ax.set(title=name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n",
    "\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_bool==True:\n",
    "        saveplot(plt, plotname, pubres=True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_boolean=False #change to True to save the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_model_dict_list= [mimic_model_recal,\n",
    "                      tert_model,\n",
    "                      pooled_model_recal_tert,\n",
    "                      ensemble_tert\n",
    "    ]\n",
    "\n",
    "clf_list = [\n",
    "    (mimic_model_recal['model'], \"MIMIC\"),\n",
    "    (tert_model['model'], \"NM-T\"),\n",
    "    (pooled_model_recal_tert['model'], \"Pooled\"),\n",
    "    (ensemble_tert['model'], \"Ensemble\"),   \n",
    "]\n",
    "\n",
    "cal_plot(X_test= x_nmedw_com, y_test= y_nmedw_com, clf_list=clf_list, save_bool=save_boolean, plotname='nm_com_calplot_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_model_dict_list= [mimic_model,\n",
    "                      tert_model_recal,\n",
    "                      pooled_model_recal_mim,\n",
    "                      ensemble_mimic\n",
    "    ]\n",
    "\n",
    "clf_list_mim = [\n",
    "    (mimic_model['model'], \"MIMIC\"),\n",
    "    (tert_model_recal['model'], \"NM-T\"),\n",
    "    (pooled_model_recal_mim['model'], \"Pooled\"),\n",
    "    (ensemble_mimic['model'], \"Ensemble\"),\n",
    "    \n",
    "]\n",
    "\n",
    "cal_plot(X_test= x_mimic_test, y_test= y_mimic_test, clf_list=clf_list_mim, save_bool=save_boolean, plotname='mim_calplot_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_model_dict_list= [mimic_model_recal,\n",
    "                      tert_model,\n",
    "                      pooled_model_recal_tert,\n",
    "                      ensemble_tert\n",
    "    ]\n",
    "\n",
    "clf_list = [\n",
    "    (mimic_model_recal['model'], \"MIMIC\"),\n",
    "    (tert_model['model'], \"NM-T\"),\n",
    "    (pooled_model_recal_tert['model'], \"Pooled\"),\n",
    "    (ensemble_tert['model'], \"Ensemble\"),\n",
    "    \n",
    "]\n",
    "\n",
    "cal_plot(X_test= x_nmedw_test, y_test= y_nmedw_test, clf_list=clf_list, save_bool=save_boolean, plotname='nm_tert_calplot_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "def z_cal(oi,Ei):\n",
    "    from scipy.stats import norm\n",
    "    from math import sqrt\n",
    "    \"\"\"\n",
    "    Spiegelhalter’s z-score neatly drops out the unavoidable penalty\n",
    "    term by taking the difference of the score with the expectation.\n",
    "    Schematically it is defined as\n",
    "\n",
    "    \n",
    "    \n",
    "    If |Z(E, O)| > q1 − α/2⁠, where qα is the \n",
    "    α-quantile of the standard normal distribution (0.05), \n",
    "    the result is significant, suggesting an improperly calibrated model.\n",
    "    \"\"\"\n",
    "    alpha = 0.05\n",
    "    \n",
    "    y_tert_z1= sum(\n",
    "        (oi-Ei) * (1-(2*Ei))\n",
    "    )\n",
    "    y_tert_z2= sqrt(sum(\n",
    "        ((1-(2*Ei))**2) * (Ei*(1-Ei))\n",
    "\n",
    "    ))\n",
    "    z_score=y_tert_z1/y_tert_z2\n",
    "    if (abs(z_score) > norm.cdf(1-(alpha/2))):\n",
    "        print('reject null. NOT calibrated')\n",
    "    else:\n",
    "        print('fail to reject. calibrated')\n",
    "        \n",
    "    print('z score: ', z_score, '\\n')\n",
    "    print('p value: ', 1-norm.cdf(abs(z_score)), '\\n')\n",
    "    return(z_score)\n",
    "\n",
    "\n",
    "def calibration_calcs(model, dataset_x, dataset_y):\n",
    "    from scipy.stats import linregress\n",
    "    import math\n",
    "   \n",
    "    oi=dataset_y['0']\n",
    "    Ei= model.predict_proba(dataset_x)[:,1]\n",
    "\n",
    "\n",
    "\n",
    "#     print('Average absolute error: {}'.format(AAE(oi=oi,Ei=Ei)))\n",
    "    print('Brier score: {}'.format(brier_score_loss(y_true=oi, y_prob=Ei)))\n",
    "#     print(brier_score_loss(y_true=oi, y_prob=Ei))\n",
    "    print('Spiegelhalter’s z-test:')\n",
    "    print(z_cal(oi=oi,Ei=Ei))\n",
    "\n",
    "    print(linregress(x=Ei, y=oi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_list = [\n",
    "    (mimic_model_recal['model'], \"mimic\"),\n",
    "    (tert_model['model'], \"nm_tertiary\"),\n",
    "    (pooled_model_recal_tert['model'], \"pooled_tertcal\"),\n",
    "    (ensemble_tert['model'], \"ensemble\")    \n",
    "]\n",
    "\n",
    "i=0\n",
    "for element in clf_list:\n",
    "    print('######### MODEL: {} #########'.format(clf_list[i][1]))\n",
    "    calibration_calcs(clf_list[i][0], dataset_x=x_nmedw_com, dataset_y=y_nmedw_com)\n",
    "    print()\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=mimic_model['model'].predict_proba(x_mimic_test)[:,1]\n",
    "m2=tert_model_recal['model'].predict_proba(x_mimic_test)[:,1]\n",
    "m3=pooled_model_recal_mim['model'].predict_proba(x_mimic_test)[:,1]\n",
    "m4=ensemble_mimic['model'].predict_proba(x_mimic_test)[:,1]\n",
    "\n",
    "t1=mimic_model_recal['model'].predict_proba(x_nmedw_test)[:,1]\n",
    "t2=tert_model['model'].predict_proba(x_nmedw_test)[:,1]\n",
    "t3=pooled_model_recal_tert['model'].predict_proba(x_nmedw_test)[:,1]\n",
    "t4=ensemble_tert['model'].predict_proba(x_nmedw_test)[:,1]\n",
    "\n",
    "c1=mimic_model_recal['model'].predict_proba(x_nmedw_com)[:,1]\n",
    "c2=tert_model['model'].predict_proba(x_nmedw_com)[:,1]\n",
    "c3=pooled_model_recal_tert['model'].predict_proba(x_nmedw_com)[:,1]\n",
    "c4=ensemble_tert['model'].predict_proba(x_nmedw_com)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_mean_list=[]\n",
    "m_std_list=[]\n",
    "for element in [m1, m2, m3, m4]:\n",
    "    m_mean_list.append(np.mean(element))\n",
    "    m_std_list.append(np.std(element))\n",
    "    \n",
    "t_mean_list=[]\n",
    "t_std_list=[]\n",
    "for element in [t1, t2, t3, t4]:\n",
    "    t_mean_list.append(np.mean(element))\n",
    "    t_std_list.append(np.std(element))\n",
    "    \n",
    "c_mean_list=[]\n",
    "c_std_list=[]\n",
    "for element in [c1, c2, c3, c4]:\n",
    "    c_mean_list.append(np.mean(element))\n",
    "    c_std_list.append(np.std(element))\n",
    "    \n",
    "pd.DataFrame(index=['MIMICD', 'TertiaryD', 'NM-CommunityD'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(np.transpose([m_mean_list,t_mean_list,c_mean_list]))\n",
    "display(pd.DataFrame(data=([m_mean_list,t_mean_list,c_mean_list]),\n",
    "            index=['MIMICD', 'TertiaryD', 'NM-CommunityD'],\n",
    "            columns=['MIMICm','Tertiarym','Pooledm','Ensemblem']).round(decimals=2))\n",
    "\n",
    "display(pd.DataFrame(data=([m_std_list,t_std_list,c_std_list]),\n",
    "            index=['MIMICD', 'TertiaryD', 'NM-CommunityD'],\n",
    "            columns=['MIMICm','Tertiarym','Pooledm','Ensemblem']).round(decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacked roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_neighbours(value, df, colname):\n",
    "    exactmatch = df[df[colname] == value]\n",
    "    if not exactmatch.empty:\n",
    "        return exactmatch.index\n",
    "    else:\n",
    "        lowerneighbour_ind = df[df[colname] < value][colname].idxmax()\n",
    "        upperneighbour_ind = df[df[colname] > value][colname].idxmin()\n",
    "        return [lowerneighbour_ind, upperneighbour_ind]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def roc_publishing(model, x, y, proba_input=False,pos_label=1, print_default=True, model_name=None):\n",
    "    import sklearn.metrics as metrics\n",
    "    from sklearn.metrics import precision_score, roc_auc_score, f1_score, recall_score\n",
    "    \n",
    "    model_name=type(model).__name__\n",
    "\n",
    "    y_proba = model.predict_proba(x)[:,1]\n",
    "        \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, y_proba, pos_label=pos_label)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    roc_df= pd.DataFrame({\"thresholds\": thresholds,\"fpr\":fpr, \"tpr\": tpr})\n",
    "    roc_df.iloc[0,0] =1\n",
    "    \n",
    "    return(fpr, tpr, roc_auc, roc_df)\n",
    "\n",
    "\n",
    "\n",
    "def stacked_roc(x_test, y_test, models_dic, first_bold=True, save_bool=False, plotname='stacked_roc'):\n",
    "    \"\"\"\n",
    "    plotting function to plot a stacked ROC based on models in a dictionary. \n",
    "    first_bold=True means that the first model in the dic will stand out and be a solid line, while others are dotted\n",
    "    \"\"\"\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set(font_scale=2)\n",
    "    plt.style.use('seaborn-white')\n",
    "    plt.rcParams['figure.figsize'] = [16, 10]\n",
    "\n",
    "    for model_name in models_dic.keys():\n",
    "\n",
    "        model=models_dic[model_name]['model']\n",
    "        fpr, tpr, roc_auc, roc_df= roc_publishing(model, x=np.array(x_test), y=y_test, model_name=model_name)\n",
    "        print(model_name, roc_auc)\n",
    "        ax1= plt.plot(fpr, tpr, label = '{}'.format(model_name)+'$_{\\mathrm{M}}$'+ ' AUC = {:.3f}'.format(roc_auc), linestyle='solid', linewidth=3, alpha=0.75)\n",
    "\n",
    "#         ##labeling the point w/ tuned high sensitivity threshold\n",
    "#         idx=find_neighbours(value=models_dic[model_name]['threshold'], df=roc_df, colname='thresholds')[0]   \n",
    "        \n",
    "#         ### comment this out to remove the threshold values:\n",
    "#         plt.plot(roc_df.iloc[idx,1], roc_df.iloc[idx,2],marker='o', markersize=8, color='black') ##\n",
    "        \n",
    "    ###annotating the plot\n",
    "    plt.legend(loc = 'lower right')   \n",
    "\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate', size=32)\n",
    "    plt.xlabel('False Positive Rate', size=32)\n",
    "    plt.grid(color='grey', linestyle='-', linewidth=1, alpha=0.2)\n",
    "\n",
    "    if save_bool==True:\n",
    "        saveplot(plt,plotname, pubres=True)\n",
    "    else: pass\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_model_dict_list= [mimic_model_recal,\n",
    "                      tert_model,\n",
    "                      pooled_model_recal_tert,\n",
    "                      ensemble_tert,\n",
    "    ]\n",
    "\n",
    "keys= ['MIMIC', 'NM-T', 'Pooled', 'Ensemble']\n",
    "com_dic={}\n",
    "for i in range(0,len(com_model_dict_list)):\n",
    "    com_dic.update({keys[i] : com_model_dict_list[i]} )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROC for models on NMEDW Community Hospital Data')\n",
    "stacked_roc(x_test=x_nmedw_com, y_test=y_nmedw_com, models_dic=com_dic, first_bold=False, save_bool=save_boolean, plotname='stacked_roc_com_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mim_model_dict_list= [mimic_model,\n",
    "                      tert_model_recal,\n",
    "                      pooled_model_recal_mim,\n",
    "                      ensemble_mimic,\n",
    "    ]\n",
    "\n",
    "keys= ['MIMIC', 'NM-T', 'Pooled', 'Ensemble']\n",
    "mim_model_dic={}\n",
    "for i in range(0,len(mim_model_dict_list)):\n",
    "    mim_model_dic.update({keys[i] : mim_model_dict_list[i]} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROC for models on MIMIC test Data')\n",
    "stacked_roc(x_test=x_mimic_test, y_test=y_mimic_test, models_dic=mim_model_dic, first_bold=False, save_bool=save_boolean, plotname='stacked_roc_mim_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_roc(x_test=x_nmedw_test, y_test=y_nmedw_test, models_dic=com_dic, first_bold=False, save_bool=save_boolean, plotname='stacked_roc_tert_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# delong test for ROC comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "# AUC comparison adapted from\n",
    "# https://github.com/Netflix/vmaf/\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"Fast DeLong test computation.\n",
    "\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight=None):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions):\n",
    "    \"\"\"Computes ROC AUC variance for a single set of predictions.\n",
    "\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "\n",
    "    \"\"\"\n",
    "    sample_weight = None\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "\n",
    "def delong_roc_test(ground_truth, predictions_one, predictions_two):\n",
    "    \"\"\"Computes log(p-value) for hypothesis that two ROC AUCs are different.\n",
    "    ** modified to return non-logscaled p-value\n",
    "\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions_one: predictions of the first model,\n",
    "          np.array of floats of the probability of being class 1\n",
    "       predictions_two: predictions of the second model,\n",
    "          np.array of floats of the probability of being class 1\n",
    "\n",
    "    \"\"\"\n",
    "    order, label_1_count, _ = compute_ground_truth_statistics(ground_truth)\n",
    "    predictions_sorted_transposed = np.vstack((predictions_one, predictions_two))[:, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "    return 10**calc_pvalue(aucs, delongcov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##### NMEDW Tertiary: #####')\n",
    "print('mimic v tertiary: {}'.format(\n",
    "    delong_roc_test(y_nmedw_test['0'].values, mimic_model_recal['model'].predict_proba(x_nmedw_test)[:,1], tert_model['model'].predict_proba(x_nmedw_test)[:,1])[0][0])\n",
    ")\n",
    "\n",
    "print('mimic v pooled: {}'.format(\n",
    "    delong_roc_test(y_nmedw_test['0'].values, pooled_model_recal_tert['model'].predict_proba(x_nmedw_test)[:,1], mimic_model_recal['model'].predict_proba(x_nmedw_test)[:,1])[0][0])\n",
    "     )\n",
    "print('mimic v ensemble: {}'.format(\n",
    "    delong_roc_test(y_nmedw_test['0'].values, ensemble_tert['model'].predict_proba(x_nmedw_test)[:,1], mimic_model_recal['model'].predict_proba(x_nmedw_test)[:,1])[0][0])\n",
    "     )\n",
    "\n",
    "print('tertiary v pooled: {}'.format(\n",
    "    delong_roc_test(y_nmedw_test['0'].values, pooled_model_recal_tert['model'].predict_proba(x_nmedw_test)[:,1], tert_model['model'].predict_proba(x_nmedw_test)[:,1])[0][0])\n",
    "     )\n",
    "print('tertiary v ensemble: {}'.format(\n",
    "    delong_roc_test(y_nmedw_test['0'].values, tert_model['model'].predict_proba(x_nmedw_test)[:,1], ensemble_tert['model'].predict_proba(x_nmedw_test)[:,1])[0][0])\n",
    "     )\n",
    "\n",
    "\n",
    "print('pooled v ensemble: {}'.format(\n",
    "    delong_roc_test(y_nmedw_test['0'].values, pooled_model_recal_tert['model'].predict_proba(x_nmedw_test)[:,1], ensemble_tert['model'].predict_proba(x_nmedw_test)[:,1])[0][0])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##### MIMIC: #####')\n",
    "print('mimic v tertiary: {}'.format(\n",
    "    delong_roc_test(y_mimic_test['0'].values, mimic_model['model'].predict_proba(x_mimic_test)[:,1], tert_model_recal['model'].predict_proba(x_mimic_test)[:,1])[0][0])\n",
    ")\n",
    "\n",
    "print('mimic v pooled: {}'.format(\n",
    "    delong_roc_test(y_mimic_test['0'].values, pooled_model_recal_mim['model'].predict_proba(x_mimic_test)[:,1], mimic_model['model'].predict_proba(x_mimic_test)[:,1])[0][0])\n",
    "     )\n",
    "print('mimic v ensemble: {}'.format(\n",
    "    delong_roc_test(y_mimic_test['0'].values, ensemble_mimic['model'].predict_proba(x_mimic_test)[:,1], mimic_model['model'].predict_proba(x_mimic_test)[:,1])[0][0])\n",
    "     )\n",
    "\n",
    "print('tertiary v pooled: {}'.format(\n",
    "    delong_roc_test(y_mimic_test['0'].values, pooled_model_recal_mim['model'].predict_proba(x_mimic_test)[:,1], tert_model_recal['model'].predict_proba(x_mimic_test)[:,1])[0][0])\n",
    "     )\n",
    "print('tertiary v ensemble: {}'.format(\n",
    "    delong_roc_test(y_mimic_test['0'].values, tert_model_recal['model'].predict_proba(x_mimic_test)[:,1], ensemble_mimic['model'].predict_proba(x_mimic_test)[:,1])[0][0])\n",
    "     )\n",
    "\n",
    "\n",
    "print('pooled v ensemble: {}'.format(\n",
    "    delong_roc_test(y_mimic_test['0'].values, pooled_model_recal_mim['model'].predict_proba(x_mimic_test)[:,1], ensemble_mimic['model'].predict_proba(x_mimic_test)[:,1])[0][0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##### NMEDW Community: #####')\n",
    "print('mimic v tertiary: {}'.format(\n",
    "    delong_roc_test(y_nmedw_com['0'].values, mimic_model_recal['com_proba'], tert_model['com_proba'])[0][0])\n",
    ")\n",
    "\n",
    "print('mimic v pooled: {}'.format(\n",
    "    delong_roc_test(y_nmedw_com['0'].values, pooled_model_recal_tert['com_proba'], mimic_model_recal['com_proba'])[0][0])\n",
    "     )\n",
    "print('mimic v ensemble: {}'.format(\n",
    "    delong_roc_test(y_nmedw_com['0'].values, mimic_model_recal['com_proba'], ensemble_tert['com_proba'])[0][0])\n",
    "     )\n",
    "\n",
    "print('tertiary v pooled: {}'.format(\n",
    "    delong_roc_test(y_nmedw_com['0'].values, pooled_model_recal_tert['com_proba'], tert_model['com_proba'])[0][0])\n",
    "     )\n",
    "print('tertiary v ensemble: {}'.format(\n",
    "    delong_roc_test(y_nmedw_com['0'].values, tert_model['com_proba'], ensemble_tert['com_proba'])[0][0])\n",
    "     )\n",
    "\n",
    "\n",
    "print('pooled v ensemble: {}'.format(\n",
    "    delong_roc_test(y_nmedw_com['0'].values, pooled_model_recal_tert['com_proba'], ensemble_tert['com_proba'])[0][0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision vs. Recall plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def roc_publishing(model, x, y, proba_input=False,pos_label=1, print_default=True, model_name=None):\n",
    "    import sklearn.metrics as metrics\n",
    "    from sklearn.metrics import precision_score, roc_auc_score, f1_score, recall_score\n",
    "\n",
    "    model_name=type(model).__name__\n",
    "\n",
    "    y_proba = model.predict_proba(x)[:,1]\n",
    "        \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, y_proba, pos_label=pos_label)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    #gathering the optimal youden_index and df of tpr/fpr for auc and index of that optimal youden. idx is needed in the roc\n",
    "    youden_threshold, roc_df, idx= optimal_youden_index(fpr, tpr, thresholds, tp90=True)\n",
    "    \n",
    "    return(fpr, tpr, roc_auc, roc_df, idx)\n",
    "\n",
    "\n",
    "\n",
    "def pr_publishing(model_dic, x, y, pos_label=1, print_default=True, model_name=None):\n",
    "    import sklearn.metrics as metrics\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "    model=model_dic['model']\n",
    "    \n",
    "    model_name=type(model).__name__\n",
    "\n",
    "    y_proba = model.predict_proba(x)[:,1]\n",
    "    y_pred=[1 if y >= model_dic['threshold'] else 0 for y in y_proba]\n",
    "    \n",
    "    lr_precision, lr_recall, thresholds = metrics.precision_recall_curve(y, y_proba, pos_label=pos_label)\n",
    "    lr_f1 = f1_score(y, y_pred)\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "\n",
    "    return(lr_precision, lr_recall,thresholds, lr_f1, lr_auc)\n",
    "\n",
    "\n",
    "    \n",
    "def stacked_pr(x_test, y_test, models_dic, save_bool=False, plotname='stacked_pr'):\n",
    "    \"\"\"\n",
    "    plotting function to plot a stacked ROC based on models in a dictionary. \n",
    "    first_bold=True means that the first model in the dic will stand out and be a solid line, while others are dotted\n",
    "    \"\"\"\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "#     global save_boolean\n",
    "    sns.set(font_scale=2)\n",
    "    plt.style.use('seaborn-white')\n",
    "    plt.rcParams['figure.figsize'] = [16, 10]\n",
    "    \n",
    "    for model_name in models_dic.keys():\n",
    "        model=models_dic[model_name]['model']\n",
    "        precision, recall, thresholds, f1, auc= pr_publishing(models_dic[model_name], x=np.array(x_test), y=y_test, model_name=model_name)\n",
    "        print(model_name, auc, f1)\n",
    "        ax1= plt.plot(precision, recall, label = '{}'.format(model_name)+'$_{\\mathrm{M}}$'+ ' AUC = {:.3f}   F1 = {:.3f}'.format(auc, f1), linestyle='solid', linewidth=3, alpha=0.75)\n",
    "        \n",
    "        ###annotating the plot\n",
    "    plt.legend(loc = 'lower left')  \n",
    "\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Precision', size=32)\n",
    "    plt.xlabel('Recall', size=32)\n",
    "    plt.grid(color='grey', linestyle='-', linewidth=1, alpha=0.2)\n",
    "    \n",
    "    if save_bool==True:\n",
    "        saveplot(plt,plotname, pubres=True)\n",
    "    else: pass\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_model_dict_list= [mimic_model_recal,\n",
    "                      tert_model,\n",
    "                      pooled_model_recal_tert,\n",
    "                      ensemble_tert\n",
    "    ]\n",
    "\n",
    "keys= ['MIMIC', 'NM-T', 'Pooled', 'Ensemble']\n",
    "com_dic={}\n",
    "for i in range(0,len(com_model_dict_list)):\n",
    "    com_dic.update({keys[i] : com_model_dict_list[i]} )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_pr(x_test=x_nmedw_com, y_test=y_nmedw_com['0'], models_dic=com_dic, save_bool=save_boolean, plotname='stacked_pr_com_final')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_pr(x_test=x_mimic_test, y_test=y_mimic_test['0'], models_dic=mim_model_dic, save_bool=save_boolean, plotname='stacked_pr_mim_final')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_pr(x_test=x_nmedw_test, y_test=y_nmedw_test['0'], models_dic=com_dic, save_bool=save_boolean, plotname='stacked_pr_tert_final')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variable importance exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### uses permutation based importances for all now\n",
    "def common_imp_variables(x_train, y_train, models_dic):\n",
    "    \"\"\"\n",
    "    function that takes in a dictionary of models and the x_train dataframe and returns the set of variables present in the combined list of each model's top N most important variables.\n",
    "    said another way, takes each model's top N variables (via permutation importance) and returns the set of all variables. \n",
    "    1) find top N variables for each model\n",
    "    2) make list of all models top N\n",
    "    3) filter to only unique values in list = imp_var_names\n",
    "    \n",
    "    note: The purpose of this function is to run the permutation variable importance w/ the entire predictor space only twice, then once the top important variables across models are found (next function), can run permutation again focused only on them.\n",
    "    this saves significant computational time.\n",
    "    \"\"\"\n",
    "    global n_varimp\n",
    "    features_dic={}\n",
    "    top_set_dic={}\n",
    "\n",
    "    for model_name in models_dic.keys():\n",
    "        model= models_dic[model_name]\n",
    "        print(model_name)\n",
    "        \n",
    "        from sklearn.inspection import permutation_importance\n",
    "        result = permutation_importance(\n",
    "            model, x_train.values, y_train, n_repeats=2, random_state=12345, n_jobs=-1\n",
    "        )\n",
    "\n",
    "        feature_names= list(x_train)\n",
    "        forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "        features=forest_importances.nlargest(n_varimp).sort_values()\n",
    "        features=list(features.reset_index()['index'])\n",
    "        features_dic.update( {model_name :features } )\n",
    "\n",
    "    #######\n",
    "    set_features=[]\n",
    "\n",
    "    for features in features_dic.values():\n",
    "        set_features=set_features+features\n",
    "    set_features=set(set_features)\n",
    "    imp_var_names=list(set_features)\n",
    "\n",
    "    return(imp_var_names)\n",
    "\n",
    "\n",
    "#### uses permutation based importances for all now\n",
    "\n",
    "def perm_impvar_calc(models_dic, imp_var_names, x_train, y_train):\n",
    "    \"\"\"\n",
    "    calculates the relative variable importance of the set of each model's topN important variables on the supplied x and y data. \n",
    "    input:\n",
    "        models_dic:dictionary of models \n",
    "        imp_var_names: the top N set of important variables among all models\n",
    "        x_train: desired predictor matrix to permute importance on\n",
    "        y_train: desired predictor matrix to permute importance on\n",
    "    output: \n",
    "        relative variable importance for each model of all set(imp_var_names) variables.\n",
    "    note: relative variable importance determined by dividing each variable importance by the value of the most important variable. this makes all values a comparison to the most important varaible:\n",
    "    ie 50 rel variable importance = half as important as the most important variable\n",
    "    \"\"\"\n",
    "    \n",
    "    # finding the index of the set(varimp_names) in the dataframe.  \n",
    "    #getting index of the top variables in the predictor matrix. \n",
    "    xtrain_column_index_list=[]\n",
    "    for element in imp_var_names:\n",
    "        variable_index=list(x_train).index(element)\n",
    "        xtrain_column_index_list.append(variable_index)\n",
    "    \n",
    "    rel_result_dic={} #instantiating dictionary\n",
    "    result_dic={}\n",
    "    for model_name in models_dic.keys():\n",
    "        model= models_dic[model_name]\n",
    "        print(model_name)           \n",
    "        from sklearn.inspection import permutation_importance\n",
    "        result = permutation_importance(\n",
    "            model, x_train.values, y_train, n_repeats=10, random_state=12345, n_jobs=-1\n",
    "        )\n",
    "\n",
    "        feature_names= list(x_train)\n",
    "        imp = pd.Series(result.importances_mean, index=feature_names)[imp_var_names]\n",
    "        imp=imp.sort_values()\n",
    "        rel_imp=100.0 * (imp / imp.max())\n",
    "        \n",
    "        features =list(np.array(x_train.columns)[xtrain_column_index_list])\n",
    "        top_set= rel_imp\n",
    "        rel_result_dic.update( {model_name :top_set } ) #scaled to be relative\n",
    "        result_dic.update( {model_name :result } ) #not scaled to be relative\n",
    "\n",
    "    return(rel_result_dic, result_dic)\n",
    "\n",
    "\n",
    "def perm_impvar_plot(result_set_dic_p, imp_var_names,models_dic, xvar_rotation=80, sort_base='nm_tertiary', figsize=[8,5], save_bool=False, plotname='perm_varimp'):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        result_set_dic_p: permuted variable importance dictionary from topN_rel_imp2(),\n",
    "        imp_var_names: top important variables to plot,\n",
    "        models_dic: ,\n",
    "        xvar_rotation: rotation of the xvariable label on the plot\n",
    "        sort_base: name of the model to provide sorting basis.\n",
    "        figsize: dimensions for figure output,\n",
    "        save_bool: boolean to save plot using saveplot function\n",
    "        plotname: name of plot used in saveplot function. \n",
    "    output:\n",
    "        importance_df: dataframe of calculated relative variable importances for additional plotting \n",
    "        error_df: same but w/ calculated permutation stdev (scaled to the ratio of value:error prior to making importances relative. )\n",
    "   \n",
    "    \"\"\"\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    feature_names= list(x_nmedw_train)\n",
    "    \n",
    "    SMALL_SIZE = 8+4\n",
    "    MEDIUM_SIZE = 10+4\n",
    "    BIGGER_SIZE = 12+4\n",
    "\n",
    "    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "    plt.rcParams['figure.figsize'] = figsize \n",
    "    \n",
    "    imp_dict={}\n",
    "    err_dict={}\n",
    "\n",
    "    for model_name in models_dic.keys(): ##now that we have set of top N variables for each model. we can make relative importance for all unique variables in the set\n",
    "        \n",
    "        model= models_dic[model_name]\n",
    "        print(model_name)\n",
    "        \n",
    "        result= result_set_dic_p[model_name]\n",
    "        \n",
    "        forest_importances = pd.Series(result.importances_mean, index=feature_names)[varimp_names]\n",
    "        forest_errors=  pd.Series(result.importances_std, index=feature_names)[varimp_names]\n",
    "        \n",
    "        ###sorting\n",
    "        forest_importances=forest_importances.sort_values()\n",
    "        forest_errors=forest_errors[forest_importances.index]\n",
    "        \n",
    "        # scaing valuse to be relative values\n",
    "        rel_imp=100.0 * (forest_importances / forest_importances.max())\n",
    "        # scaling errors to be the same ratio of value:error pre-scaling\n",
    "        forest_errors_scale= forest_errors/forest_importances\n",
    "        rel_imp_errors= forest_errors_scale * rel_imp\n",
    "        \n",
    "        imp_dict.update({model_name :rel_imp })\n",
    "        err_dict.update({model_name :rel_imp_errors })\n",
    "        \n",
    "        \n",
    "    importance_df=pd.DataFrame(imp_dict)\n",
    "    error_df=pd.DataFrame(err_dict)\n",
    "\n",
    "    importance_df=abs(importance_df.sort_values(sort_base))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    importance_df.plot.bar(yerr=error_df.loc[importance_df.index], ax=ax, capsize=4, rot=80)\n",
    "    \n",
    "    if save_bool==True:\n",
    "        saveplot(plt,plotname)\n",
    "       \n",
    "    plt.show()\n",
    "    return(importance_df,error_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_impvar_plot_prep(result_set_dic_p, imp_var_names,models_dic, sort_base='Ensemble'):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        result_set_dic_p: permuted variable importance dictionary from topN_rel_imp2(),\n",
    "        imp_var_names: top important variables to plot,\n",
    "        models_dic: ,\n",
    "        sort_base: name of the model to provide sorting basis.\n",
    "    output:\n",
    "        importance_df: dataframe of calculated relative variable importances for additional plotting \n",
    "        error_df: same but w/ calculated permutation stdev (scaled to the ratio of value:error prior to making importances relative. )\n",
    "   \n",
    "    \"\"\"\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    feature_names= list(x_nmedw_train)\n",
    "    \n",
    "    imp_dict={}\n",
    "    err_dict={}\n",
    "\n",
    "    for model_name in models_dic.keys(): ##now that we have set of top N variables for each model. we can make relative importance for all unique variables in the set\n",
    "        model= models_dic[model_name]\n",
    "        print(model_name)\n",
    "        \n",
    "        result= result_set_dic_p[model_name]\n",
    "        \n",
    "        forest_importances = pd.Series(result.importances_mean, index=feature_names)[imp_var_names]\n",
    "        forest_errors=  pd.Series(result.importances_std, index=feature_names)[imp_var_names]\n",
    "        \n",
    "        ###sorting\n",
    "        forest_importances=forest_importances.sort_values()\n",
    "        forest_errors=forest_errors[forest_importances.index]\n",
    "        \n",
    "        # scaing valuse to be relative values\n",
    "        rel_imp=100.0 * (forest_importances / forest_importances.max())\n",
    "        # scaling errors to be the same ratio of value:error pre-scaling\n",
    "        forest_errors_scale= forest_errors/forest_importances\n",
    "        rel_imp_errors= forest_errors_scale * rel_imp\n",
    "        \n",
    "        imp_dict.update({model_name :rel_imp })\n",
    "        err_dict.update({model_name :rel_imp_errors })\n",
    "        \n",
    "    importance_df=pd.DataFrame(imp_dict)\n",
    "    error_df=pd.DataFrame(err_dict)\n",
    "\n",
    "    importance_df=abs(importance_df.sort_values(sort_base))\n",
    "    return(importance_df,error_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_impvar_plot(importance_df, error_df, x_var_labels, sort_base='Ensemble', xvar_rotation=80, figsize=[8,5], save_bool=False, plotname='perm_varimp'):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        importance_df: dataframe of calculated relative variable importances for additional plotting \n",
    "        error_df: same but w/ calculated permutation stdev (scaled to the ratio of value:error prior to making importances relative.\n",
    "        xvar_rotation: rotation of the xvariable label on the plot\n",
    "        figsize: dimensions for figure output,\n",
    "        save_bool: boolean to save plot using saveplot function\n",
    "        plotname: name of plot used in saveplot function. \n",
    "\n",
    "   \n",
    "    \"\"\"\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    feature_names= list(x_nmedw_train)\n",
    "    \n",
    "    SMALL_SIZE = 8+4\n",
    "    MEDIUM_SIZE = 10+4\n",
    "    BIGGER_SIZE = 12+4\n",
    "\n",
    "    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "    plt.rcParams['figure.figsize'] = figsize \n",
    "    \n",
    "\n",
    "\n",
    "    importance_df=importance_df.sort_values('Ensemble', ascending=False).iloc[0:10,:]\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    importance_df.plot.bar(yerr=error_df.loc[importance_df.index], ax=ax, capsize=4, rot=xvar_rotation)\n",
    "    plt.grid(color='grey', linestyle='-', linewidth=1, alpha=0.2)\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "\n",
    "    for i in range(len(x_var_labels)):\n",
    "        labels[i] = x_var_labels[i]\n",
    "\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylim([0, 120])\n",
    "    ax.set_ylabel(\"Relative Variable Importance\", size=25)\n",
    "\n",
    "    if save_bool==True:\n",
    "        saveplot(plt,plotname)\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### setting up a model dictionary list in the expected format for the relative variable plotting functions. \n",
    "com_model_dict_list= [mimic_model_recal['model'],\n",
    "                      tert_model['model'],\n",
    "                      pooled_model_recal_tert['model'],\n",
    "                      ensemble_tert['model']\n",
    "    ]\n",
    "\n",
    "keys= ['MIMIC', 'Tertiary', 'Pooled', 'Ensemble']\n",
    "com_dic={}\n",
    "for i in range(0,len(com_model_dict_list)):\n",
    "    com_dic.update({keys[i] : com_model_dict_list[i]} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## warning this code takes 30-60 min to run since it is permuting n=10 replicates on a model that is already in a double nested ensemble (due to the recalibrationCV). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permuting variable importances for all 4 models on Tertiary_train. \n",
    "n_varimp=10\n",
    "\n",
    "#find set(topN) variables\n",
    "imp_var_names= common_imp_variables(x_train=x_nmedw_train, y_train=y_nmedw_train, models_dic=com_dic) ##n varimp names not important for the df\n",
    "\n",
    "#find rel importance of set(topN) variables for each model\n",
    "rel_perm_imp_var_result, raw_perm_imp_var_result= perm_impvar_calc(models_dic=com_dic, imp_var_names=imp_var_names, x_train=x_nmedw_train, y_train=y_nmedw_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find rel importance of set(topN) variables for each model\n",
    "# rel_perm_imp_var_result, raw_perm_imp_var_result= perm_impvar_calc(models_dic=com_dic, imp_var_names=imp_var_names, x_train=x_nmedw_train, y_train=y_nmedw_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df, error_df= perm_impvar_plot_prep(raw_perm_imp_var_result, imp_var_names, models_dic=com_dic, sort_base='Ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.sort_values('Ensemble', ascending=False).iloc[0:12,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df2=importance_df.drop(index=['minWBC','bicarbonate','bilirubin','mingcs','resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df2.sort_values('Ensemble', ascending=False).iloc[0:10,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var_label_list=[\n",
    "                'Temperature (max)',\n",
    "                 'Blood Culture (+)',\n",
    "                 'Leukocyte (+)',\n",
    "                 'BUN (max)',\n",
    "                 'Heartrate (max)',\n",
    "                 'WBC (max)',\n",
    "                 'PaO2:FiO2 (min)',\n",
    "                 'Norepinephrine (+)',\n",
    "                 'Respirate (max)',\n",
    "                 'SBP (min)'\n",
    "                ] #list of  cleaned up variable label names for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###published varimp plot. \n",
    "perm_impvar_plot(importance_df2, error_df, x_var_labels=x_var_label_list, xvar_rotation=80, figsize=[20,10], save_bool=False, plotname='perm_varimp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# membership model and assessing similarity between development and validation cohorts\n",
    "https://www.sciencedirect.com/science/article/pii/S0895435614002753?via%3Dihub\n",
    ". We hereto estimate a binary logistic regression model, further referred to as membership model, to predict the probability that an individual belongs to (is a member of) the development sample as compared with the validation sample. Hence, the dependent variable of this model is “1” for participants of the development set and “0” for those of the validation set. This model should at least include as independent variables the predictors and outcome from the original prediction model to ensure that model performance can (at least partially) be interpreted in terms of its considered predictors and outcome. It may be clear that if the membership model discriminates poorly (or well), both samples are strongly (or not much) related in terms of the considered predictor variables and outcome status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nmedw_train_m= y_nmedw_train.copy()\n",
    "y_nmedw_train_m['0']=1\n",
    "\n",
    "y_mimic_train_m=y_mimic_train.copy()\n",
    "y_mimic_train_m['0']=1\n",
    "\n",
    "y_nmedw_com_m= y_nmedw_com.copy()\n",
    "y_nmedw_com_m['0']=0\n",
    "\n",
    "# y_mimic_train_m=y_mimic_train.copy()\n",
    "# y_mimic_train_m['0']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_m_1= pd.concat([x_nmedw_train, x_nmedw_com]) \n",
    "\n",
    "x_m_2= pd.concat([x_mimic_train, x_nmedw_com]) \n",
    "\n",
    "y_m_1= pd.concat([y_nmedw_train_m, y_nmedw_com_m])\n",
    "\n",
    "y_m_2= pd.concat([y_mimic_train_m, y_nmedw_com_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_m_1= pd.concat([x_nmedw_train, x_nmedw_com]) \n",
    "\n",
    "x_m_2= pd.concat([x_mimic_train, x_nmedw_com]) \n",
    "\n",
    "y_m_1= pd.concat([y_nmedw_train_m, y_nmedw_com_m])\n",
    "\n",
    "y_m_2= pd.concat([y_mimic_train_m, y_nmedw_com_m])\n",
    "\n",
    "lr_model_m_1= LogisticRegression(random_state=12345,solver='liblinear', penalty='l1')\n",
    "lr_model_m_1.fit(x_m_1,y_m_1)\n",
    "\n",
    "lr_model_m_2= LogisticRegression(random_state=12345,solver='liblinear', penalty='l1')\n",
    "lr_model_m_2.fit(x_m_2,y_m_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rf: cat model on cat testset (balanced)\n",
    "m_1_eval= classifier_eval(lr_model_m_1, x=np.array(x_m_1), y=y_m_1,\n",
    "                         training=False,train_threshold= 0.5,\n",
    "                         model_name='m', folder_name=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rf: cat model on cat testset (balanced)\n",
    "m_2_eval= classifier_eval(lr_model_m_2, x=np.array(x_m_2), y=y_m_2,\n",
    "                         training=False,train_threshold= 0.5,\n",
    "                         model_name='m', folder_name=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model should at least include as independent variables the predictors and outcome from the original prediction model to ensure that model performance can (at least partially) be interpreted in terms of its considered predictors and outcome. \n",
    "## it hink this means i should include my y variable as a predictor?\n",
    "\n",
    "x_nmedw_train_m=x_nmedw_train.copy()\n",
    "x_nmedw_train_m['y']=y_nmedw_train['0']\n",
    "\n",
    "x_mimic_train_m=x_mimic_train.copy()\n",
    "x_mimic_train_m['y']=y_mimic_train['0']\n",
    "\n",
    "x_nmedw_com_m=x_nmedw_com.copy()\n",
    "x_nmedw_com_m['y']=y_nmedw_com['0']\n",
    "\n",
    "\n",
    "x_m_1= pd.concat([x_nmedw_train_m, x_nmedw_com_m]) \n",
    "\n",
    "x_m_2= pd.concat([x_mimic_train_m, x_nmedw_com_m]) \n",
    "\n",
    "y_m_1= pd.concat([y_nmedw_train_m, y_nmedw_com_m])\n",
    "\n",
    "y_m_2= pd.concat([y_mimic_train_m, y_nmedw_com_m])\n",
    "\n",
    "lr_model_m_1= LogisticRegression(random_state=12345,solver='liblinear', penalty='l2')\n",
    "lr_model_m_1.fit(x_m_1,y_m_1)\n",
    "\n",
    "lr_model_m_2= LogisticRegression(random_state=12345,solver='liblinear', penalty='l2')\n",
    "lr_model_m_2.fit(x_m_2,y_m_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pooled_train_m=x_pooled_train2.copy()\n",
    "x_pooled_train_m['y']=y_pooled_train2['0']\n",
    "\n",
    "y_pooled_train_m= y_pooled_train2.copy()\n",
    "y_pooled_train_m['0']=1\n",
    "\n",
    "x_m_3= pd.concat([x_pooled_train_m, x_nmedw_com_m]) \n",
    "y_m_3= pd.concat([y_pooled_train_m, y_nmedw_com_m])\n",
    "\n",
    "\n",
    "lr_model_m_3= LogisticRegression(random_state=12345,solver='liblinear', penalty='l2')\n",
    "lr_model_m_3.fit(x_m_3,y_m_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_nmedw_train_m= y_nmedw_train.copy()\n",
    "# y_nmedw_train_m['0']=1\n",
    "\n",
    "y_mimic_train_m2=y_mimic_train.copy()\n",
    "y_mimic_train_m2['0']=0\n",
    "\n",
    "\n",
    "x_m_4= pd.concat([x_nmedw_train_m, x_mimic_train_m]) \n",
    "y_m_4= pd.concat([y_nmedw_train_m, y_mimic_train_m2])\n",
    "\n",
    "\n",
    "lr_model_m_4= LogisticRegression(random_state=12345,solver='liblinear', penalty='l2')\n",
    "lr_model_m_4.fit(x_m_4,y_m_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rf: cat model on cat testset (balanced)\n",
    "m_1_eval= classifier_eval(lr_model_m_1, x=np.array(x_m_1), y=y_m_1,\n",
    "                         training=False,train_threshold= 0.5,\n",
    "                         model_name='m', folder_name=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rf: cat model on cat testset (balanced)\n",
    "m_2_eval= classifier_eval(lr_model_m_2, x=np.array(x_m_2), y=y_m_2,\n",
    "                         training=False,train_threshold= 0.5,\n",
    "                         model_name='m', folder_name=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rf: cat model on cat testset (balanced)\n",
    "m_3_eval= classifier_eval(lr_model_m_3, x=np.array(x_m_3), y=y_m_3,\n",
    "                         training=False,train_threshold= 0.5,\n",
    "                         model_name='m', folder_name=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### development model between MIMIC and NM-T\n",
    "m_4_eval= classifier_eval(lr_model_m_4, x=np.array(x_m_4), y=y_m_4,\n",
    "                         training=False,train_threshold= 0.5,\n",
    "                         model_name='m', folder_name=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doing the same thing between NM-T train vs test and MIMIC train vs test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_nmedw_train_m=x_nmedw_train.copy()\n",
    "x_nmedw_train_m['y']=y_nmedw_train['0']\n",
    "\n",
    "x_nmedw_test_m=x_nmedw_test.copy()\n",
    "x_nmedw_test_m['y']=y_nmedw_test['0']\n",
    "\n",
    "\n",
    "y_nmedw_train_m= y_nmedw_train.copy()\n",
    "y_nmedw_train_m['0']=1\n",
    "\n",
    "y_nmedw_test_m= y_nmedw_test.copy()\n",
    "y_nmedw_test_m['0']=0\n",
    "\n",
    "\n",
    "\n",
    "x_mimic_train_m=x_mimic_train.copy()\n",
    "x_mimic_train_m['y']=y_mimic_train['0']\n",
    "\n",
    "x_mimic_test_m=x_mimic_test.copy()\n",
    "x_mimic_test_m['y']=y_mimic_test['0']\n",
    "\n",
    "\n",
    "y_mimic_train_m= y_mimic_train.copy()\n",
    "y_mimic_train_m['0']=1\n",
    "\n",
    "y_mimic_test_m= y_mimic_test.copy()\n",
    "y_mimic_test_m['0']=0\n",
    "\n",
    "\n",
    "\n",
    "x_m_11= pd.concat([x_nmedw_train_m, x_nmedw_test_m]) \n",
    "\n",
    "x_m_22= pd.concat([x_mimic_train_m, x_mimic_test_m]) \n",
    "\n",
    "y_m_11= pd.concat([y_nmedw_train_m, y_nmedw_test_m])\n",
    "\n",
    "y_m_22= pd.concat([y_mimic_train_m, y_mimic_test_m])\n",
    "\n",
    "lr_model_m_11= LogisticRegression(random_state=12345,solver='liblinear', penalty='l2')\n",
    "lr_model_m_11.fit(x_m_11,y_m_11)\n",
    "\n",
    "lr_model_m_22= LogisticRegression(random_state=12345,solver='liblinear', penalty='l2')\n",
    "lr_model_m_22.fit(x_m_22,y_m_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### development model between MIMIC and NM-T\n",
    "m_11_eval= classifier_eval(lr_model_m_11, x=np.array(x_m_11), y=y_m_11,\n",
    "                         training=False,train_threshold= 0.5,\n",
    "                         model_name='m', folder_name=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### development model between MIMIC and NM-T\n",
    "m_22_eval= classifier_eval(lr_model_m_22, x=np.array(x_m_22), y=y_m_22,\n",
    "                         training=False,train_threshold= 0.5,\n",
    "                         model_name='m', folder_name=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## membership model variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### the membership models to be explored for variable importance:\n",
    "# ### rf: membership model between M:nm-t and nm-C. 0.82\n",
    "# m_1_eval= classifier_eval(lr_model_m_1, x=np.array(x_m_1), y=y_m_1,\n",
    "#                          training=False,train_threshold= 0.5,\n",
    "#                          model_name='m', folder_name=folder)\n",
    "\n",
    "# ### rf:  membership model between MIMIC and nm-C. 0.98\n",
    "# m_2_eval= classifier_eval(lr_model_m_2, x=np.array(x_m_2), y=y_m_2,\n",
    "#                          training=False,train_threshold= 0.5,\n",
    "#                          model_name='m', folder_name=folder)\n",
    "\n",
    "# ### rf:  membership model between POOLED and nm-C. 0.98\n",
    "# m_3_eval= classifier_eval(lr_model_m_3, x=np.array(x_m_3), y=y_m_3,\n",
    "#                          training=False,train_threshold= 0.5,\n",
    "#                          model_name='m', folder_name=folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def memb_importance(lr_model,x_m, y_m):\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    model_fi = permutation_importance(lr_model, x_m, y_m, n_repeats=20, random_state=12345, n_jobs=-1)\n",
    "\n",
    "    model_fi1=pd.DataFrame(data=model_fi['importances_mean'], columns=['rel_imp'])#, scoring= accuracy by default. aka the average decrease in accuracy when variable is permutted.  \n",
    "    model_fi1['colname']= lr_model.feature_names_in_\n",
    "    model_fi1['rel_imp2']=abs(model_fi1['rel_imp'])\n",
    "    model_fi1=model_fi1.drop(columns='rel_imp').rename(columns={'rel_imp2':'imp'})\n",
    "    model_fi1['imp_std']=model_fi['importances_std']\n",
    "    model_fi1=model_fi1.sort_values('imp', ascending=False)#.reset_index(drop=True)\n",
    "    model_fi1['rel_imp']= 100*(model_fi1['imp']/model_fi1['imp'].max())\n",
    "    return(model_fi1)\n",
    "\n",
    "def memb_importance_auc(lr_model,x_m, y_m):\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    model_fi = permutation_importance(lr_model, x_m, y_m,  scoring=['roc_auc'], n_repeats=20, random_state=12345, n_jobs=-1)\n",
    "\n",
    "    model_fi1=pd.DataFrame(data=model_fi['roc_auc']['importances_mean'], columns=['rel_imp'])#, columns= \n",
    "    model_fi1['colname']= lr_model.feature_names_in_\n",
    "    model_fi1['rel_imp2']=abs(model_fi1['rel_imp'])\n",
    "    model_fi1=model_fi1.drop(columns='rel_imp').rename(columns={'rel_imp2':'imp'})\n",
    "    model_fi1['imp_std']=model_fi['roc_auc']['importances_std']\n",
    "    model_fi1=model_fi1.sort_values('imp', ascending=False)#.reset_index(drop=True)\n",
    "    model_fi1['rel_imp']= 100*(model_fi1['imp']/model_fi1['imp'].max())\n",
    "    return(model_fi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importance = average decrease in ACCURACY across n=10 permutations\n",
    "nmt_nmc_imp=memb_importance(lr_model_m_1, x_m_1, y_m_1)\n",
    "mim_nmc_imp=memb_importance(lr_model_m_2, x_m_2, y_m_2)\n",
    "pool_nmc_imp=memb_importance(lr_model_m_3, x_m_3, y_m_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importance = average decrease in AUROC across n=10 permutations\n",
    "nmt_nmc_imp2=memb_importance_auc(lr_model_m_1, x_m_1, y_m_1)\n",
    "mim_nmc_imp2=memb_importance_auc(lr_model_m_2, x_m_2, y_m_2)\n",
    "pool_nmc_imp2=memb_importance_auc(lr_model_m_3, x_m_3, y_m_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memb_impvar_names=list(set(nmt_nmc_imp.head(6)['colname'].to_list()+ mim_nmc_imp.head(6)['colname'].to_list()+ pool_nmc_imp.head(6)['colname'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###making relative errors\n",
    "nmt_nmc_imp['rel_std']= nmt_nmc_imp['rel_imp']*(nmt_nmc_imp['imp_std']/nmt_nmc_imp['imp'])\n",
    "mim_nmc_imp['rel_std']= mim_nmc_imp['rel_imp']*(mim_nmc_imp['imp_std']/mim_nmc_imp['imp'])\n",
    "pool_nmc_imp['rel_std']= pool_nmc_imp['rel_imp']*(pool_nmc_imp['imp_std']/pool_nmc_imp['imp'])\n",
    "\n",
    "\n",
    "nmt_nmc_imp_prep= nmt_nmc_imp.loc[nmt_nmc_imp['colname'].isin(memb_impvar_names),['colname','rel_imp','rel_std']]\n",
    "mim_nmc_imp_prep= mim_nmc_imp.loc[mim_nmc_imp['colname'].isin(memb_impvar_names),['colname','rel_imp','rel_std']]\n",
    "pool_nmc_imp_prep= pool_nmc_imp.loc[pool_nmc_imp['colname'].isin(memb_impvar_names),['colname','rel_imp','rel_std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_dic={\n",
    "    'Tertiary':nmt_nmc_imp_prep.set_index('colname')['rel_imp'],\n",
    "    'MIMIC':mim_nmc_imp_prep.set_index('colname')['rel_imp'],\n",
    "    'Pooled':pool_nmc_imp_prep.set_index('colname')['rel_imp'],\n",
    "}\n",
    "\n",
    "member_error_dic={\n",
    "    'Tertiary':nmt_nmc_imp_prep.set_index('colname')['rel_std'],\n",
    "    'MIMIC':mim_nmc_imp_prep.set_index('colname')['rel_std'],\n",
    "    'Pooled':pool_nmc_imp_prep.set_index('colname')['rel_std'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df=pd.DataFrame(member_dic).sort_values('Tertiary', ascending=False)#.reset_index()\n",
    "err_df= pd.DataFrame(member_error_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "colors =sns.color_palette()[0:3]\n",
    "my_cmap = ListedColormap(sns.color_palette())#.as_hex())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "colors=ListedColormap(sns.color_palette('deep')).colors\n",
    "\n",
    "xvar_rotation=80\n",
    "figsize=[8,5]\n",
    "\n",
    "# feature_names= list(x_nmedw_train)\n",
    "x_var_labels=list(imp_df.index)\n",
    "\n",
    "SMALL_SIZE = 8+4\n",
    "MEDIUM_SIZE = 10+4\n",
    "BIGGER_SIZE = 12+4\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams['figure.figsize'] = figsize \n",
    "\n",
    "imp_df=imp_df.sort_values('Tertiary', ascending=False).iloc[0:11,:]\n",
    "imp_df = imp_df.rename_axis(None) #making sure the axis column title is removed\n",
    "x_var_labels=list(imp_df.index)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "imp_df.plot.bar(yerr=err_df.loc[imp_df.index], ax=ax, capsize=4, rot=xvar_rotation, color= [colors[0],colors[1],colors[2]])#, c=colors)\n",
    "\n",
    "plt.grid(color='grey', linestyle='-', linewidth=1, alpha=0.2)\n",
    "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "\n",
    "for i in range(len(x_var_label_list)):\n",
    "    labels[i] = x_var_label_list[i]\n",
    "\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_ylim([0, 120])\n",
    "ax.set_ylabel(\"Relative Variable Importance\", size=15)\n",
    "\n",
    "saveplot(plt,'membership_importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling result heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_summary_exp1_df.reset_index())\n",
    "display(test_summary_exp2_df.reset_index())\n",
    "display(test_summary_exp3_df.reset_index())\n",
    "display(test_summary_exp4_df)\n",
    "display(test_com_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_model_dict_list= [mimic_model_recal,\n",
    "                      tert_model,\n",
    "                      pooled_model_recal_tert,\n",
    "                      ensemble_tert,\n",
    "    ]\n",
    "\n",
    "keys= ['MIMIC', 'NM-T', 'Pooled', 'Ensemble']\n",
    "com_dic={}\n",
    "for i in range(0,len(com_model_dict_list)):\n",
    "    com_dic.update({keys[i] : com_model_dict_list[i]} )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm=pd.DataFrame({'MIMIC_m':[0.782, 0.722,0.741],\n",
    "                  'NM-T_m':[0.694,0.810,0.807],\n",
    "                   'Pooled_m':[0.774,0.788, 0.795],\n",
    "                  'Ensemble_m':[0.767,0.798, 0.798 ]\n",
    "                  }\n",
    "            ).transpose()#,     \n",
    "hm=hm.rename(columns={0:'MIMICtest',1:'NM-Ttest',2:'NM-Cval'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(font_scale=2.5)\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]   \n",
    "\n",
    "x_axis_labels=[]\n",
    "y_axis_labels=[]\n",
    "\n",
    "for element in ['MIMIC', 'NM-T', 'Pooled', 'Ensemble']:\n",
    "    y_axis_labels.append('{}'.format(element)+'$_{\\mathrm{M}}$')\n",
    "    \n",
    "for element in ['MIMIC', 'NM-T']:\n",
    "    x_axis_labels.append('{}'.format(element)+'$_{\\mathrm{test}}$')\n",
    "x_axis_labels.append('{}'.format('NM-C')+'$_{\\mathrm{val}}$')\n",
    "\n",
    "ax = sns.heatmap(hm, annot=True,xticklabels=x_axis_labels, \n",
    "                 yticklabels=y_axis_labels,\n",
    "                 cmap=sns.cubehelix_palette(start=.2, rot=-.3, as_cmap=True,dark=0.25, light=.75, reverse=True))\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "\n",
    "ax.xaxis.tick_top()\n",
    "plt.tick_params(left = False, bottom = False, top=False)\n",
    "\n",
    "# saveplot(plt,'auc_heatmap_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comln_tert_model = label_bi_risk(x_com= x_nmedw_com_longneg,\n",
    "                y_com= y_nmedw_com_longneg, icu_com= icu_nmedw_com_longneg, \n",
    "                model_in=tert_model , model_threshold= 0.2671307538155534, \n",
    "                pt_df=final_pt_df2)\n",
    "comln_mim_model = label_bi_risk(x_com= x_nmedw_com_longneg,\n",
    "                y_com= y_nmedw_com_longneg, icu_com= icu_nmedw_com_longneg, \n",
    "                model_in=mim_model , model_threshold= 0.274, \n",
    "                pt_df=final_pt_df2)\n",
    "\n",
    "comln_ensemble_model = label_bi_risk(x_com= x_nmedw_com_longneg,\n",
    "                y_com= y_nmedw_com_longneg, icu_com= icu_nmedw_com_longneg, \n",
    "                model_in=ensemble_tert_model , model_threshold= 0.267, \n",
    "                pt_df=final_pt_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting function displayed in supplement\n",
    "sns.set(rc={'figure.figsize':(10,8)}, font_scale = 1.3)\n",
    "sns.set_style(\"white\")\n",
    "colors=sns.color_palette('deep').as_hex()\n",
    "\n",
    "sns.distplot(comln_mim_model['y_proba_model'], hist=False, kde=True, color=colors[0],norm_hist=False, hist_kws=dict(alpha=1),label='MIMIC'+'$_{\\mathrm{M}}$')\n",
    "\n",
    "sns.distplot(comln_tert_model['y_proba_model'], hist=False, kde=True, color=colors[1],norm_hist=False, hist_kws=dict(alpha=0.5),label='NM-T'+'$_{\\mathrm{M}}$')\n",
    "\n",
    "sns.distplot(comln_ensemble_model['y_proba_model'], hist=False, kde=True, color=colors[2],norm_hist=False, hist_kws=dict(alpha=0.5),label='Ensemble'+'$_{\\mathrm{M}}$')\n",
    "\n",
    "plt.axvline(x=0.274, color='black',  linestyle='dashed', label='Prediction Threshold')\n",
    "plt.legend(['MIMIC'+'$_{\\mathrm{M}}$','NM-T'+'$_{\\mathrm{M}}$','Ensemble'+'$_{\\mathrm{M}}$','Prediction Threshold'])\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "display(comln_mim_model['y_hat_model'].value_counts())\n",
    "display(comln_tert_model['y_hat_model'].value_counts())\n",
    "display(comln_ensemble_model['y_hat_model'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bringing some fairness stats in:\n",
    "black_bool=x_nmedw_com['ethnicity_black']==1\n",
    "hispanic_bool= x_nmedw_com['ethnicity_hispanic']==1\n",
    "other_bool= x_nmedw_com['ethnicity_unknown/other']==1\n",
    "female_bool= x_nmedw_com['gender_M']==0\n",
    "\n",
    "\n",
    "x=np.array(x_nmedw_com.copy())\n",
    "y=y_nmedw_com.copy() #copy of y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### modified classifier_eval function to output additional information useful for fairness analysis\n",
    "\n",
    "def classifier_eval(model,\n",
    "                    x,\n",
    "                    y,\n",
    "                    proba_input=False,\n",
    "                    pos_label=1,\n",
    "                    training=True,\n",
    "                    train_threshold=None,\n",
    "                    print_default=True,\n",
    "                    model_name=None,\n",
    "                    folder_name=None,\n",
    "                    save=save_boolean):\n",
    "    import sklearn.metrics as metrics\n",
    "    from sklearn.metrics import precision_score, roc_auc_score, f1_score, recall_score\n",
    "    \"\"\"\n",
    "    classification evaluation function. able to print/save the following:\n",
    "    \n",
    "    print/save the following:\n",
    "        ROC curve marked with threshold for optimal youden (maximizing tpr+fpr with constraint that tpr>0.9)\n",
    "\n",
    "        using 0.5 threshold:\n",
    "            confusion matrix\n",
    "            classification report\n",
    "            npv\n",
    "            accuracy\n",
    "\n",
    "        using optimal youden (maximizing tpr+fpr with constraint that tpr>0.9):\n",
    "            confusion matrix\n",
    "            classification report\n",
    "            npv\n",
    "            accuracy\n",
    "    \n",
    "    output: \n",
    "        outputs modelname, auc, precision, recall, f1, and npv to a dictionary. \n",
    "    \n",
    "    notes:\n",
    "    youden's J statistic:\n",
    "    J= sensitivity + specificity -1\n",
    "    (truepos/ truepos+falseneg) + (true neg/ trueneg + falsepos) -1. \n",
    "    NOTE: with tpr>0.9 turned on, the youden statistic is basically just the furthest point on the line away from the midline with tpr>=0.9\n",
    "    NOTE2: this function arguably does too much. in the future it may be better to seperate it out into more compartmental functions like with preprocessing().\n",
    "    \"\"\"\n",
    "    \n",
    "    if proba_input==True: \n",
    "        y_proba= model\n",
    "        y_pred=[1 if y >= 0.5 else 0 for y in y_proba]\n",
    "    \n",
    "    else:\n",
    "        model_name=type(model).__name__\n",
    "\n",
    "        y_pred = model.predict(x)\n",
    "        y_proba = model.predict_proba(x)[:,1]\n",
    "        \n",
    "    if training==True:\n",
    "        \n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, y_proba, pos_label=pos_label)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        #gathering the optimal youden_index and df of tpr/fpr for auc and index of that optimal youden. idx is needed in the roc\n",
    "        tp_threshold, roc_df, idx= optimal_youden_index(fpr, tpr, thresholds,tp90=True)\n",
    "    \n",
    "    else: #if training is not true, then we use the tuned threshold specified on the trainingset.\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, y_proba, pos_label=pos_label)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "        roc_df= pd.DataFrame({\"thresholds\": thresholds,\"fpr\":fpr, \"tpr\": tpr})\n",
    "        roc_df.iloc[0,0] =1\n",
    "        tp_threshold= train_threshold\n",
    "\n",
    "    #plotting roc\n",
    "    #plot_roc(fpr, tpr, roc_auc, threshold, save=save_boolean,model_name=None, folder_name=None, file_name=None\n",
    "    plot_roc(fpr, tpr, roc_auc, thresholds, tp_threshold, save=save_boolean, model_name=model_name,folder_name=folder)\n",
    "    plt.show(), plt.close()\n",
    "    \n",
    "    #printing npv, recall, precision, accuracy\n",
    "    npv=confusion_matrix(y, y_pred)[0,0]/sum(np.array(y_pred)==0)\n",
    "    prec= precision_score(y_true=y, y_pred= y_pred, pos_label=pos_label)\n",
    "    recall= recall_score(y_true=y, y_pred= y_pred, pos_label=pos_label)\n",
    "    f1= f1_score(y_true=y, y_pred= y_pred, pos_label=pos_label)\n",
    "    confusion =pd.DataFrame(confusion_matrix(y, y_pred),\n",
    "                                 index=['condition_neg','condition_pos'],\n",
    "                                columns=['pred_neg','pred_pos'])\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    \n",
    "    if save==True:\n",
    "        save_df(confusion, df_name='{}_confusion_base'.format(model_name), rel_path='/tables/', verbose=False)\n",
    "    \n",
    "    if print_default==True: ###can opt to not print the 0.5 classification threshold classification report/conf matrix\n",
    "        #plotting confusion matrixs\n",
    "        print(\"\\n******* Using 0.5 Classification Threshold *******\\n\")\n",
    "        print(confusion)\n",
    "        print('\\n')\n",
    "        print ('the Accuracy is: {:01.3f}'.format(accuracy_score(y, y_pred)))\n",
    "        print (\"npv: {:01.3f}\".format(npv))\n",
    "        print ('the classification_report:\\n', classification_report(y,y_pred, digits=3))\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #### YOUDEN ADJUSTMENT #####\n",
    "\n",
    "    print(\"\\n******* Using Optimal TPR>=0.9 Classification Threshold *******\\n\")\n",
    "    print(\"\\nthe Youden optimal index is : {:01.3f}\".format(train_threshold))\n",
    "\n",
    "    y_pred_youden = [1 if y >= train_threshold else 0 for y in y_proba]\n",
    "\n",
    "    npv_y=confusion_matrix(y, y_pred_youden)[0,0]/sum(np.array(y_pred_youden)==0)\n",
    "    prec_y= precision_score(y_true=y, y_pred= y_pred_youden, pos_label=pos_label)\n",
    "    recall_y= recall_score(y_true=y, y_pred= y_pred_youden, pos_label=pos_label)\n",
    "    f1_y= f1_score(y_true=y, y_pred= y_pred_youden, pos_label=pos_label)\n",
    "    auc_y=roc_auc_score(y_true=y, y_score= y_proba)\n",
    "    \n",
    "    \n",
    "    ##plotting and saving confusion matrix\n",
    "    confusion_youden=pd.DataFrame(confusion_matrix(y, y_pred_youden),\n",
    "                                 index=['condition_neg','condition_pos'],\n",
    "                                columns=['pred_neg','pred_pos'])\n",
    "    \n",
    "    if save==True:\n",
    "        save_df(confusion_youden, df_name='{}_confusion_tuned'.format(model_name), rel_path='/tables/',verbose=False)\n",
    "    \n",
    "    #plotting confusion matrixs\n",
    "    print('\\n')\n",
    "    print(confusion_youden)\n",
    "    print('\\n')\n",
    "    print ('the Accuracy is: {:01.3f}'.format(accuracy_score(y, y_pred_youden)))\n",
    "    print (\"npv: {:01.3f}\".format(npv_y))\n",
    "    print ('the classification_report:\\n', classification_report(y,y_pred_youden, digits=3))\n",
    "    \n",
    "    youden_dic= {'model':model_name, 'auc':auc_y, 'precision':prec_y, 'recall':recall_y, 'f1':f1_y, 'npv':npv_y,'threshold':tp_threshold}\n",
    "    return(youden_dic, confusion_youden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_analysis(group_bool):\n",
    "\n",
    "    x_interest_group= x_nmedw_com[group_bool]\n",
    "    y_interest_group= y_nmedw_com[group_bool]\n",
    "\n",
    "    x_opinterest_group=x_nmedw_com[~group_bool]\n",
    "    y_opinterest_group= y_nmedw_com[~group_bool]\n",
    "    \n",
    "    \n",
    "    print('### TERT MODEL ON NM-C interest group')\n",
    "    rf_tert_group_interest,rf_tert_group_interest2 = classifier_eval(tert_model, x=np.array(x_interest_group), y=y_interest_group,\n",
    "                             training=False,train_threshold= 0.267,\n",
    "                             model_name='rf', folder_name=folder, save=False, print_default=False)\n",
    "    \n",
    "    rf_tert_group_interest['model']='NM-T'\n",
    "    rf_tert_group_interest['group']='interest'\n",
    "    \n",
    "    print('### TERT MODEL ON NM-C opinterest group')\n",
    "    rf_tert_group_opinterest,rf_tert_group_opinterest2= classifier_eval(tert_model, x=np.array(x_opinterest_group), y=y_opinterest_group,\n",
    "                             training=False,train_threshold= 0.267,\n",
    "                             model_name='rf', folder_name=folder, save=False, print_default=False)\n",
    "    rf_tert_group_opinterest['model']='NM-T'\n",
    "    rf_tert_group_opinterest['group']='opinterest'\n",
    "    \n",
    "    print('### MIMIC MODEL ON NM-C interest group')\n",
    "    rf_mimic_group_interest,rf_mimic_group_interest2= classifier_eval(mim_model, x=np.array(x_interest_group), y=y_interest_group,\n",
    "                             training=False,train_threshold= 0.274,\n",
    "                             model_name='rf', folder_name=folder, save=False, print_default=False)\n",
    "    rf_mimic_group_interest['model']='MIMIC'\n",
    "    rf_mimic_group_interest['group']='interest'\n",
    "    \n",
    "    print('### MIMIC MODEL ON NM-C opinterest group')\n",
    "    rf_mimic_group_opinterest,rf_mimic_group_opinterest2= classifier_eval(mim_model, x=np.array(x_opinterest_group), y=y_opinterest_group,\n",
    "                             training=False,train_threshold= 0.274,\n",
    "                             model_name='rf', folder_name=folder, save=False, print_default=False)\n",
    "    rf_mimic_group_opinterest['model']='MIMIC'\n",
    "    rf_mimic_group_opinterest['group']='opinterest'\n",
    "        \n",
    "    summary_df= pd.DataFrame([rf_tert_group_interest, rf_tert_group_opinterest, rf_mimic_group_interest, rf_mimic_group_opinterest])\n",
    "    summary_df=summary_df.round(decimals=3)#.sort_values('auc', ascending=False)\n",
    "    group_results= {'tert_interest':rf_tert_group_interest2, 'tert_opinterest':rf_tert_group_opinterest2, 'mim_interest':rf_mimic_group_interest2, 'mim_opinterest':rf_mimic_group_opinterest2}\n",
    "    summary_df=summary_df[['model','group','auc','f1','npv','precision','recall','threshold']]\n",
    "\n",
    "    return(summary_df, group_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fairness_analysis_manual(results, bool_name):\n",
    "    \"\"\"\n",
    "    function to manually calculate statistics related to fiarness\n",
    "    \"\"\"\n",
    "\n",
    "    d1=results['tert_interest'] #underpriv\n",
    "    d2=results['tert_opinterest'] #priv\n",
    "    n_female=sum(d1.sum())\n",
    "    n_male=sum(d2.sum())\n",
    "    n_tot=len(bool_name)\n",
    "    \n",
    "    print('baseline disadvantaged prevalence: {:.3f}'.format(n_female/n_tot))\n",
    "    print('baseline disadvantaged condition pos prev: {:.3f}'.format(d1.loc['condition_pos'].sum()/n_female))\n",
    "    print('baseline advantaged condition pos prev: {:.3f}'.format(d2.loc['condition_pos'].sum()/n_male))\n",
    "    \n",
    "    # correct # w/ adjusted decision threshold\n",
    "\n",
    "    print('######### TERTm ###########')\n",
    "    \n",
    "    p1z1= d1.loc[:,'pred_pos'].sum()/n_female #predicted pos given the sensitive group = True\n",
    "    p1z0= d2.loc[:,'pred_pos'].sum()/n_male #predicted pos given the sensitive group = False\n",
    "\n",
    "    p1z1y1= d1.loc['condition_pos','pred_pos'].sum()/n_female\n",
    "    p1z0y1= d2.loc['condition_pos','pred_pos'].sum()/n_male\n",
    "    \n",
    "    p1=min((p1z1/p1z0),(p1z0/p1z1))\n",
    "    eod1=min((p1z1y1/p1z0y1),(p1z0y1/p1z1y1))\n",
    "    \n",
    "    TPR1=  d1.loc['condition_pos','pred_pos']/ d1.loc['condition_pos'].sum()\n",
    "    TPR2=  d2.loc['condition_pos','pred_pos']/ d2.loc['condition_pos'].sum()\n",
    "    \n",
    "    print('p% score: {:.3f}'.format(p1))\n",
    "\n",
    "    print('equality of opportunity: {:.3f}'.format(eod1))\n",
    "    print('equality of opportunity diff: {:.3f}'.format(TPR2-TPR1))\n",
    "    print((p1z1y1/p1z0y1), (p1z0y1/p1z1y1))\n",
    "    \n",
    "    print('######### MIMICm ###########')\n",
    "    # correct # w/ adjusted decision threshold\n",
    "    d1=results['mim_interest']\n",
    "    d2=results['mim_opinterest']\n",
    "\n",
    "    n_female=sum(d1.sum())\n",
    "    n_male=sum(d2.sum())\n",
    "    n_tot=len(bool_name)\n",
    "\n",
    "\n",
    "    p1z1= d1.loc[:,'pred_pos'].sum()/n_female #predicted pos given the sensitive group = True\n",
    "    p1z0= d2.loc[:,'pred_pos'].sum()/n_male #predicted pos given the sensitive group = False\n",
    "\n",
    "    p1z1y1= d1.loc['condition_pos','pred_pos'].sum()/n_female\n",
    "    p1z0y1= d2.loc['condition_pos','pred_pos'].sum()/n_male\n",
    "\n",
    "    p2=min((p1z1/p1z0),(p1z0/p1z1))\n",
    "    eod2=min((p1z1y1/p1z0y1),(p1z0y1/p1z1y1))\n",
    "          \n",
    "    TPR1=  d1.loc['condition_pos','pred_pos']/ d1.loc['condition_pos'].sum()\n",
    "    TPR2=  d2.loc['condition_pos','pred_pos']/ d2.loc['condition_pos'].sum()\n",
    "    \n",
    "    print('p% score: {:.3f}'.format(p2))\n",
    "\n",
    "    print('equality of opportunity: {:.3f}'.format(eod2))\n",
    "    print('equality of opportunity diff: {:.3f}'.format(TPR2-TPR1))\n",
    "    print((p1z1y1/p1z0y1), (p1z0y1/p1z1y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_analysis_manual(black__results, black_bool)\n",
    "\n",
    "# baseline disadvantaged prevalence: 0.061\n",
    "# baseline disadvantaged condition pos prev: 0.464\n",
    "# baseline advantaged condition pos prev: 0.445\n",
    "# ######### TERTm ###########\n",
    "# p% score: 0.876\n",
    "# equality of opportunity: 0.916\n",
    "# equality of opportunity diff: 0.112\n",
    "# 0.9158486045903264 1.0918835220012328\n",
    "# ######### MIMICm ###########\n",
    "# p% score: 0.985\n",
    "# equality of opportunity: 0.986\n",
    "# equality of opportunity diff: 0.049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_analysis_manual(hispanic_results, hispanic_bool)\n",
    "# baseline disadvantaged prevalence: 0.067\n",
    "# baseline disadvantaged condition pos prev: 0.392\n",
    "# baseline advantaged condition pos prev: 0.450\n",
    "# ######### TERTm ###########\n",
    "# p% score: 0.918\n",
    "# equality of opportunity: 0.890\n",
    "# equality of opportunity diff: -0.021\n",
    "# 0.889522599793747 1.1241985310231233\n",
    "# ######### MIMICm ###########\n",
    "# p% score: 0.923\n",
    "# equality of opportunity: 0.845\n",
    "# equality of opportunity diff: 0.026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_analysis_manual(other_results, other_bool)\n",
    "# baseline disadvantaged prevalence: 0.036\n",
    "# baseline disadvantaged condition pos prev: 0.483\n",
    "# baseline advantaged condition pos prev: 0.445\n",
    "# ######### TERTm ###########\n",
    "# p% score: 0.893\n",
    "# equality of opportunity: 0.864\n",
    "# equality of opportunity diff: -0.060\n",
    "# 1.1573148712522858 0.8640690833929565\n",
    "# ######### MIMICm ###########\n",
    "# p% score: 0.996\n",
    "# equality of opportunity: 0.931\n",
    "# equality of opportunity diff: 0.010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_analysis_manual(female_results, female_bool)\n",
    "# baseline disadvantaged prevalence: 0.499\n",
    "# baseline disadvantaged condition pos prev: 0.446\n",
    "# baseline advantaged condition pos prev: 0.446\n",
    "# ######### TERTm ###########\n",
    "# p% score: 0.986\n",
    "# equality of opportunity: 0.993\n",
    "# equality of opportunity diff: 0.007\n",
    "# 0.9926281728444803 1.0074265745796787\n",
    "# ######### MIMICm ###########\n",
    "# p% score: 0.924\n",
    "# equality of opportunity: 0.928\n",
    "# equality of opportunity diff: 0.069"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
